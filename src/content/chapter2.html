<h2 id="chapter2" class="h2 p-2">Глава 2: Низкоуровневое управление памятью</h2>

<hr class="border-2">

<p class="justify-style">В предыдущей главе вы узнали теоретические основы управления памятью. Теперь вы можете сразу перейти к деталям автоматического управления памятью, как работает сборщик мусора и где могут возникать утечки памяти. Но если вы действительно хотите «освоить» эту тему, стоит потратить еще немного времени на низкоуровневые аспекты управления памятью. Это позволит вам лучше понять различные проектные решения, которые были приняты создателями сборщика мусора в .NET (а также других управляемых сред выполнения). Создатели таких механизмов не живут в вакууме и должны адаптироваться к ограничениям и механизмам, которые управляют компьютерным оборудованием и операционными системами.</p>

<p class="justify-style">Вы узнаете о этих механизмах и ограничениях в этой главе. Честно говоря, не так просто представить такие темы в не перегружающей форме. К сожалению, управление памятью в .NET было бы неполным без углубления в эти детали.</p>

<p class="justify-style">Хотя оператор «new» достаточен для большинства сценариев управления памятью в .NET, более глубокое понимание основных процессов и механизмов может оказаться полезным. Оборудование, операционная система и компилятор влияют на то, как это работает и как был написан .NET, хотя это не всегда очевидно. Эти знания очень согласуются с духом Механической Симпатии, представленной в предыдущей главе. Мы надеемся, что вам также будет просто интересно узнать некоторые из упомянутых здесь мелких фактов.</p>

<p class="justify-style">Еще раз, если вы торопитесь или просто хотите перейти к более практическим внутренним аспектам .NET и примерам, не стесняйтесь просмотреть эту главу и вернуться к ней в более свободное время, надеемся.</p>

<hr class="border-2">
<p id="chapter2-1" class="h3 p-2">Аппаратное обеспечение</p>

<p class="justify-style"> Как работает современный компьютер? Вы, вероятно, уже имеете базовое представление о предмете: компьютер состоит из процессора, который является основной вычислительной единицей – он выполняет программы. У него есть доступ к оперативной памяти (которая быстрая) и жестким дискам (которые медленные). Также есть видеокарта, которая очень важна для геймеров (и различных видов графических дизайнеров), которая отвечает за создание изображения, отображаемого на мониторе. Такой обзор с высоты птичьего полета недостаточен для наших целей. Давайте углубимся в тему. Для целей ваших размышлений давайте введем архитектуру современного компьютера, как на диаграмме на <a href="#f-2-1">Рисунке 2-1</a>.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Современный рынок персональных компьютеров доминируют ПК и Маки. Смоделированная схема архитектуры общего компьютера основана на них. При необходимости будут введены некоторые возможные нюансы, такие как те, которые касаются процессоров ARM или более сложных серверных машин.</p>
  </div>
</div>

<p class="justify-style">Основные компоненты типичной архитектуры компьютера можно перечислить как</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Процессор</span> (CPU, центральный процессор): Основной блок, отвечающий за выполнение инструкций, как описано в Главе 1. Здесь находятся такие компоненты, как арифметико-логические устройства (ALU), устройства с плавающей запятой (FPU), регистры и конвейеры выполнения инструкций, которые делят инструкции на набор более мелких операций и выполняют их, если возможно, параллельно.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Шина передней панели</span> (FSB): Шина данных, соединяющая процессор с северным мостом.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Северный мост</span>: Блок, содержащий в основном контроллер памяти, отвечающий за управление связью между памятью и процессором.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">ОЗУ</span> (оперативная память): Основная память компьютера. Она хранит данные и код программ до тех пор, пока питание включено – поэтому ее также называют динамической оперативной памятью (DRAM) или энергозависимой памятью.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Шина памяти</span>: Шина данных, соединяющая ОЗУ с северным мостом.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Южный мост</span>: Чип, который обрабатывает все функции ввода-вывода, такие как USB, аудио, последовательный порт, системный BIOS, шина ISA, контроллер прерываний и каналы IDE – контроллеры массового хранения, такие как PATA и/или SATA.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Ввод-вывод хранения</span>: Энергонезависимая память, которая хранит данные, включая популярные HDD или SSD диски.</p>
  </li>
</ul>

<figure id="f-2-1" class="figure">
  <img src="content/img/2-1.png" class="img-fluid" alt="Рисунок 2-1" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-1. Архитектура компьютера – ЦПУ, ОЗУ, северный мост, южный мост и другие. Ширина шины иллюстрирует пропорцию объема передаваемых данных (очень приблизительно)</figcaption>
</figure>

<p class="justify-style">Стоит упомянуть, что ранее процессор, северный мост и южный мост были отдельными чипами, но теперь они тесно интегрированы. Начиная с микроархитектур Intel Nehalem и AMD Zen, северный мост включен в кристалл процессора (который в таком случае часто называют <span class="fw-bold fst-italic">uncore</span> или <span class="fw-bold fst-italic">System Agent</span>). Эта эволюция архитектуры показана на <a href="#f-2-2">Рисунке 2-2</a>.</p>

<figure id="f-2-2" class="figure">
  <img src="content/img/2-2.png" class="img-fluid" alt="Рисунок 2-2" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-2. Современное оборудование – процессор с северным мостом внутри, ОЗУ, южный мост (переименованный в Platform Controller Hub в случае терминологии Intel) и другие. Ширина шины иллюстрирует пропорцию объема передаваемых данных (очень приблизительно)</figcaption>
</figure>

<p class="justify-style">Такая интеграция помогает, потому что контроллер памяти (внутри северного моста) расположен ближе к исполнительным блокам процессора, уменьшая задержки за счет меньших физических расстояний и улучшенного взаимодействия. Но на рынке все еще есть процессоры (наиболее популярные из которых – семейство AMD FX), у которых процессор, северный мост и южный мост разделены.</p>

<p class="justify-style">Основная проблема любого управления памятью заключается в несоответствии производительности современных процессоров по отношению к подсистемам памяти и массового хранения. Процессор намного быстрее памяти, поэтому каждый доступ к памяти вызывает нежелательные задержки. Когда процессору приходится ждать доступа к памяти (чтение или запись), это называется остановкой. Остановки негативно влияют на использование процессора, так как приводят к потере циклов процессора на ожидание, а не на выполнение задач.</p>

<p class="justify-style">Типичный современный процессор работает на частоте 3 ГГц или выше. Между тем, память работает с внутренними тактовыми частотами другого порядка величины, всего 200–400 МГц. Было бы слишком дорого создавать микросхемы ОЗУ, работающие на частоте процессоров. Это связано с тем, как устроены современные ОЗУ – зарядка и разрядка внутренних конденсаторов занимает время и его очень трудно уменьшить.</p>

<p class="justify-style">Вас может удивить, что память работает с такими низкими частотами. На самом деле, в компьютерных магазинах модули памяти рекламируются с частотами, такими как 3200 или 4800 МГц, которые гораздо ближе к скорости процессора. Откуда берутся такие цифры? Как вы увидите, такие спецификации – это только часть более сложной истины.</p>

<p class="justify-style">Модули памяти состоят из <span class="fw-bold fst-italic">внутренних ячеек памяти</span> (хранящих данные) и дополнительных буферов, которые помогают преодолеть их низкие внутренние тактовые частоты. Используются некоторые дополнительные приемы (см. <a href="#f-2-3">Рисунок 2-3</a>). Большинство из них основаны на умножении чтения данных:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Отправка данных из внутренней ячейки памяти дважды в течение одного тактового цикла. Для точности, это как на спаде, так и на подъеме сигнала. Отсюда и название самой популярной памяти различных поколений – <span class="fw-bold fst-italic">Double Data Rate</span> (DDR). Этот метод также называют <span class="fw-bold fst-italic">двойной накачкой</span>.</p>
  </li>
  <li>
    <p class="justify-style">Использование внутренней буферизации для выполнения нескольких чтений одновременно («режим burst») в одном тактовом цикле памяти. Это умножает количество прочитанных данных при той же внутренней частоте. Интерфейс памяти DDR2 удваивает внешнюю тактовую частоту, в то время как DDR3 и DDR4 увеличивают ее в четыре раза. DDR5 удваивает ее еще раз.</p>
  </li>
</ul>

<p class="justify-style">Эти методы в настоящее время используются в модулях DDR в отличие от гораздо более простых модулей SDRAM (синхронный DRAM), использовавшихся в прошлом. В конечном итоге, в случае современных типичных DDR5, кратность «burst» модуля памяти составляет 16, поскольку он сочетает технику двойной накачки с восемью чтениями одновременно.</p>

<figure id="f-2-3" class="figure">
  <img src="content/img/2-3.png" class="img-fluid" alt="Рисунок 2-3" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-3. Внутреннее устройство SDRAM, DDR, DDR2, DDR3, DDR4 и DDR5. Пример модулей памяти с внутренней частотой 300 МГц. MT/s означает «Мега передача в секунду». Обратите внимание, что это не строгая, а скорее иллюстративная диаграмма, показывающая соотношения между внутренними частотами и результирующими MT/s</figcaption>
</figure>

<p class="justify-style">Для иллюстрации давайте рассмотрим типичный чип памяти DDR4, например, 16 ГБ 2400 МГц (описанный в спецификациях как DDR4-2400, PC4-19200). В этих случаях внутренняя тактовая частота массива DRAM составляет 300 МГц. Тактовая частота шины памяти увеличивается в четыре раза до 1200 МГц благодаря внутреннему буферу ввода-вывода. Кроме того, происходит две передачи за каждый тактовый цикл (оба склона сигнала), что приводит к скорости передачи данных 2400 MT/s (мега передача в секунду). Отсюда и берется спецификация 2400 МГц. Проще говоря, из-за природы двойной накачки в памяти DDR, скорость обычно указывается как двойная частота тактовой шины ввода-вывода, которая сама по себе является умножением внутренней тактовой частоты памяти. Указание этого значения в МГц – это просто маркетинговое упрощение. Вторая подпись – PC4-19200 – обладает максимальной теоретической производительностью такой памяти – это 2400 МТ/с, умноженные на 8 байт (передается одно слово длиной 64 бита), что дает результат 19200 МБ/с.</p>

<p class="justify-style">Давайте рассмотрим настольный ПК Конрада в контексте всей архитектуры. Он оснащен процессором Intel Core i7-4770K (поколение Haswell), работающим на частоте 3,5 ГГц. Частота шины передней панели составляет всего 100 МГц. Используемая память DDR3-1600 (PC3-12800) имеет внутреннюю тактовую частоту памяти 200 МГц, и благодаря механизму DDR3 тактовая частота шины ввода-вывода составляет 800 МГц. Это показано на <a href="#f-2-4">Рисунке 2-4</a>. Это подтверждается использованием инструментов аппаратной диагностики, таких как CPU-Z (см. <a href="#f-2-5">Рисунок 2-5</a>).</p>

<p class="justify-style">Модули памяти постоянно улучшаются. Например, для DDR5 основным драйвером изменений было улучшение пропускной способности памяти. Вот почему была введена удвоенная длина burst, наряду с другими аналогичными изменениями, такими как удвоение количества «банков» и «групп банков» или введение двух независимых каналов вместо одного. Однако объяснение этих техник потребовало бы объяснения низкоуровневой работы модулей памяти, что выходит за рамки этой книги.</p>

<p class="justify-style">Если вам это интересно, вы можете начать с постера RAM Anatomy, доступного на сайте <a href="https://prodotnetmemory.com/">https://prodotnetmemory.com/</a>.</p>

<figure id="f-2-4" class="figure">
  <img src="content/img/2-4.png" class="img-fluid" alt="Рисунок 2-4" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-4. Современная аппаратная архитектура с дискретной тактовой частотой (Intel Core i7-4770K и DDR3-1600)</figcaption>
</figure>

<figure id="f-2-5" class="figure">
  <img src="content/img/2-5.png" class="img-fluid" alt="Рисунок 2-5" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-5. Скриншот CPU-Z – вкладка «Память», на которой показаны частоты северного моста (NB) и DRAM, а также соотношение частот FSB:DRAM (которое, к сожалению, неверно в данной версии инструмента и должно быть 1:8)</figcaption>
</figure>

<p class="justify-style">Несмотря на все описанные здесь улучшения памяти DDR, процессоры все еще намного быстрее памяти, которую они используют. Чтобы преодолеть эту проблему, применяется аналогичный подход на разных уровнях – приближение части данных к компоненту с более производительными (и более дорогими) блоками памяти. Такой подход называется кэшированием.</p>

<p class="justify-style">Для массовой памяти, такой как HDD, данные обычно кэшируются в ОЗУ – или в более быстрой, но меньшей по размеру выделенной памяти, такой как небольшой SSD внутри гибридных HDD-дисков, предназначенных для наиболее часто используемых данных. Для ОЗУ данные кэшируются внутри кэша процессора, как вы скоро увидите.</p>

<p class="justify-style">Конечно, существуют более общие оптимизации ОЗУ, включая лучшее аппаратное проектирование, лучшие контроллеры памяти и оптимизацию DMA (Direct Memory Access - прямой доступ к памяти) для устройств. Однако DMA не рассматривается в этой книге, так как он не связан напрямую с данными программы, и эти области памяти не управляются сборщиком мусора.</p>

<hr class="border-2">

<p id="chapter2-1-1" class="h4 p-2">Память</p>

<p class="justify-style">В настоящее время существует два основных типа памяти, используемых в персональных компьютерах, которые значительно различаются как по стоимости производства и использования, так и по производительности:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Статическая оперативная память</span> (SRAM): Обеспечивает очень быстрый доступ, но является довольно сложной, состоящей из 4–6 транзисторов на ячейку (хранящую один бит). Она сохраняет данные, пока питание включено, и не требует обновления. Из-за высокой скорости используется в основном в кэшах процессора.</p>
    </li>
    <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Динамическая оперативная память</span> (DRAM): Очень простая конструкция ячейки (гораздо меньше, чем у SRAM) состоит из одного транзистора и конденсатора. Из-за утечки заряда конденсатора ячейка требует постоянного обновления (что занимает драгоценные миллисекунды и замедляет чтение памяти). Сигнал, считанный с конденсатора, должен быть усилен, что усложняет процесс. Чтение и запись также занимают время и не являются линейными из-за задержек конденсатора (требуется некоторое время для получения правильного чтения или успешной записи).</p>
    </li>
</ul>

<p class="justify-style">Давайте уделим еще несколько слов технологии DRAM, так как она является основой широко используемой памяти, установленной в слотах DIMM наших компьютеров. Как уже упоминалось, одна ячейка DRAM состоит из транзистора и конденсатора и хранит один бит данных. Такие ячейки сгруппированы в массивы DRAM. Адрес для доступа к конкретной ячейке предоставляется через так называемые адресные линии.</p>

<p class="justify-style">Было бы очень сложно и дорого, если бы каждая ячейка в массиве DRAM имела свой собственный адрес. Например, в случае 32-битной адресации потребовался бы 32-битный декодер адресных линий (компонент, отвечающий за выбор конкретной ячейки). Количество адресных линий в значительной степени влияет на общую стоимость системы – чем больше линий, тем больше выводов и соединений между контроллером памяти и чипами памяти (модулями). Из-за этого адресные линии используются повторно как строки и столбцы (см. <a href="#f-2-6">Рисунок 2-6</a>), и для предоставления полного адреса требуется дважды записывать на одни и те же линии.</p>

<figure id="f-2-6" class="figure">
  <img src="content/img/2-6.png" class="img-fluid" alt="Рисунок 2-6" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-6. Пример чипа DRAM с массивом DRAM и наиболее важными каналами: адресные линии, 
    RAS и CAS</figcaption>
</figure>

<p class="justify-style">Чтение одного бита из конкретной ячейки занимает несколько шагов:</p>

<ol class="number-list ms-1">
  <li>
    <p class="justify-style">Номер строки помещается на адресные линии.</p>
    </li>
    <li>
    <p class="justify-style">Интерпретация запускается сигналом стробирования адреса строки (RAS) на выделенной линии.</p>
    </li>
    <li>
    <p class="justify-style">Номер столбца помещается на адресные линии.</p>
    </li>
    <li>
      <p class="justify-style">Интерпретация запускается сигналом стробирования адреса столбца (CAS).</p>
    </li>
    <li>
      <p class="justify-style">Строка и столбец указывают на конкретную ячейку DRAM в массиве. Один бит считывается из ячейки и записывается на линию данных.</p>
    </li>
</ol>

<p class="justify-style">Модули DRAM, установленные в наших компьютерах, состоят из множества таких массивов DRAM, организованных таким образом, чтобы мы могли получить доступ к нескольким битам (одному слову) за один тактовый цикл.</p>

<p class="justify-style">Временные интервалы перехода между отдельными шагами получения этого одного бита сильно влияют на производительность памяти. Эти временные интервалы могут быть вам знакомы, так как они являются важным фактором в спецификации модулей памяти, что сильно влияет на их цену. Вы, вероятно, знаете о таймингах модулей DIMM, таких как DDR3 9-9-9-24. Все эти тайминги указывают количество тактовых циклов, необходимых для выполнения определенных действий. Соответственно, они имеют следующие значения:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">tCL (CAS латентность): Время между стробом адреса столбца (CAS) и началом ответа (получением данных).</p>
    </li>
    <li>
    <p class="justify-style">tRCD (задержка RAS до CAS): Минимальное время между стробом адреса строки (RAS) и стробом адреса столбца (CAS).</p>
    </li>
    <li>
    <p class="justify-style">tRP (предзарядка строки): Время, необходимое для предзарядки строки перед доступом к ней. Строка не может быть использована без предварительной подготовки, называемой предзарядкой.</p>
    </li>
    <li>
    <p class="justify-style">tRAS (задержка активной строки): Минимальное время, в течение которого строка должна быть активной для доступа к информации в ней.</p>
    </li>
</ul>

<p class="justify-style">Обратите внимание на важность этих временных интервалов. Если строка и столбец, которые вас интересуют, уже установлены, считывание происходит почти мгновенно. Если вы хотите изменить столбец, это займет tCL тактовых циклов. Если вы хотите изменить строку, ситуация намного хуже: сначала она должна быть перезаряжена (tRP циклы), затем следуют задержки RAS и CAS (tCL и tRCD).</p>

<p class="justify-style">Все эти временные интервалы важны для пользователей компьютеров, ожидающих максимальной производительности. Игроки особенно обращают внимание на эти параметры. При покупке модулей памяти вы должны стремиться к минимально возможным таймингам, которые вы можете себе позволить, если производительность является вашим приоритетом.</p>

<p class="justify-style">Однако нас интересует влияние архитектуры памяти DRAM и ее таймингов на управление памятью. Стоимость изменения строки – временные интервалы сигнала RAS и перезарядка – значительна. Это одна из многих причин, почему последовательные шаблоны доступа к памяти намного быстрее, чем непоследовательные. Чтение данных в режиме burst из одной строки (изменяя столбец время от времени) намного быстрее, чем частое изменение строки. Если шаблон доступа полностью случайный, вы, скорее всего, столкнетесь с этими временными интервалами изменения строки при каждом доступе к памяти.</p>

<p class="justify-style">Вся представленная здесь информация имеет одну цель – убедиться, что у вас есть глубокая причина запомнить, почему непоследовательный доступ к памяти так нежелателен. И, как вы увидите, это не единственная причина, почему полностью случайный доступ является наихудшим сценарием.</p>

<hr class="border-2">

<p id="chapter2-1-2" class="h4 p-2">ЦПУ</p>

<p class="justify-style">Теперь перейдем к теме центрального процессора. Процессор совместим с так называемой архитектурой набора инструкций (ISA) – она определяет, среди прочего, набор операций, которые могут выполняться (инструкции), регистры и их значение, как адресуется память и так далее. В этом смысле ISA является контрактом (интерфейсом), установленным между производителем процессора и его пользователями – программами, написанными в соответствии с данным контрактом. Это уровень, который вы видите при программировании, например, на языке ассемблера данной архитектуры. ISA IA-32 (32-битные процессоры i386, Pentium 32-битные процессоры), совместимые с AMD64 (большинство современных процессоров, включая Intel Core, AMD FX и Zen и т.д.), и A64 для ARM64 являются наиболее широко используемыми в мире экосистемы .NET. Под ISA находится так называемая микроархитектура процессора, которая ее реализует. Это позволяет улучшать микроархитектуру без влияния на систему и программное обеспечение, сохраняя обратную совместимость.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Существует много путаницы с названиями стандартов 64-битной архитектуры, и вы часто можете встретить x86-64, EMT64T, Intel 64 или AMD64, используемые взаимозаменяемо. Несмотря на наличие имен производителей и иногда незначительные различия, для целей этой книги вы можете смело считать, что эти названия однозначны и могут быть безопасно заменены друг на друга.</p>
  </div>
</div>

<p class="justify-style">Как было сказано в предыдущей главе, регистры являются ключевыми компонентами ЦПУ, потому что в настоящее время все компьютеры реализованы как регистровые машины. В контексте манипуляции данными доступ к регистрам является мгновенным в том смысле, что он происходит в течение одного процессорного цикла и не вызывает дополнительных задержек. Нет места для ваших данных ближе к ЦПУ, чем регистры процессора. Конечно, регистры хранят только данные, необходимые для текущих инструкций, поэтому их нельзя считать универсальной памятью. На самом деле, в общем, процессоры имеют больше регистров, чем это видно из их ISA. Это позволяет выполнять различные типы оптимизаций (например, так называемое переименование регистров). Однако это детали реализации микроархитектуры и не влияют на механизмы управления памятью.</p>

<p class="h5 p-2">Кэш ЦПУ</p>

<p class="justify-style">Как мы уже упоминали ранее, чтобы уменьшить разрыв в производительности между ЦПУ и ОЗУ, используется 
  промежуточный компонент для хранения копий наиболее часто используемых и необходимых данных – кэш ЦПУ. В общем виде это 
  показано на <a href="#2-7">Рисунке 2-7</a>.</p>

<figure id="f-2-7" class="figure">
  <img src="content/img/2-7.png" class="img-fluid" alt="Рисунок 2-7" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-7. Взаимосвязь ЦПУ с кэшем и ОЗУ</figcaption>
</figure>

<p class="justify-style">Этот кэш прозрачен с точки зрения ISA. Ни программисту, ни операционной системе не нужно знать о ее существовании. Они не обязаны им управлять. В идеальном мире правильное использование и управление кэшем должно быть исключительной ответственностью центрального процессора.</p>

<p class="justify-style">Поскольку кэш должен быть максимально быстрым, используются ранее упомянутые чипы SRAM. Из-за своей стоимости и размера (занимающего драгоценное место в процессоре) они не могут иметь такую ​​же большую емкость, как основная оперативная память. Но в зависимости от предполагаемых затрат они могут быть такими же быстрыми, как ЦП, или может быть, только на один-два порядка медленнее.</p>

<p class="h5 p-2">Попадание и промах кэша</p>

<p class="justify-style">Идея кэша тривиальна. Когда выполняемая процессором инструкция нуждается в доступе к памяти (будь то запись или чтение), она сначала проверяет кэш, чтобы узнать, находятся ли нужные данные уже там. Если да, то отлично! Вы только что получили очень быстрый доступ к памяти, и такая ситуация называется попаданием в кэш. Если данных нет в кэше (так называемый промах кэша), то сначала их нужно прочитать из ОЗУ перед тем, как сохранить в кэш, что, очевидно, является гораздо более медленной операцией. Соотношение попаданий и промахов кэша являются очень важными показателями, показывающими, насколько эффективно наш код использует кэш.</p>

<p class="h5 p-2">Локальность данных</p>

<p class="justify-style"> Но почему такой кэш вообще полезен? Кэширование основано на очень важной концепции – <span class="fw-bold fst-italic">локальности данных</span>. Вы можете различить два вида локальности:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Временная локальность</span>: Если вы обращаетесь к какому-то региону памяти, вы, скорее всего, обратитесь к нему снова в ближайшем будущем. Это делает использование кэша вполне оправданным – вы читаете некоторые данные из памяти и, вероятно, будете использовать их позже еще несколько раз. В общем, вы загружаете некоторые структуры данных в переменные и используете эти переменные многократно (счетчики, временные данные, считанные из файлов и так далее).</p>
    </li>
    <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Пространственная локальность</span>: Если вы обращаетесь к какому-то региону памяти, вы, скорее всего, обратитесь к данным из близкого окружения. Этот тип локальности может стать вашим союзником, если вы кэшируете немного больше окружающих данных, чем вам нужно в данный момент. Например, если вам нужно несколько байт из памяти, давайте прочитаем и за кэшируем еще десяток байт. Вы редко используете очень изолированные области памяти. Вы скоро обнаружите, что стек и куча организованы таким образом, что потоки, выполняющие свою работу, обычно обращаются к похожим областям памяти. Локальные переменные или поля в структурах данных также обычно размещаются близко друг к другу.</p>
    </li>
</ul>

<p class="justify-style">Обратите внимание, что кэш полезен, если вышеупомянутые условия действительно выполняются. Однако это палка о двух концах. Если вы напишете программу таким образом, что она нарушает локальность данных, кэш станет ненужной обузой. Вы увидите это позже в главе.</p>

<p class="h5 p-2">Реализация кэша</p>

<p class="justify-style">До тех пор, пока сохраняется совместимость с моделью памяти ISA, детали реализации кэша теоретически не имеют значения. Он должен быть просто для ускорения доступа к памяти и все. Тем не менее, это прекрасный пример <span class="fw-bold fst-italic">Закона дырявых абстракций</span>, придуманного Джоэлом Спольски:</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Цитата</div>
  <div class="card-body">
    <p class="card-text justify-style">Все нетривиальные абстракции, в той или иной степени, дырявые</p>
  </div>
</div>

<p class="justify-style">Это означает, что абстракция, которая теоретически должна скрывать детали реализации, к сожалению, при определенных обстоятельствах раскрывает их наружу. И обычно это происходит непредсказуемым и/или нежелательным образом. Как это работает в случае с кэшем, должно стать ясно в ближайшее время, а пока давайте просто немного углубимся в детали реализации.</p>

<p class="justify-style">Самым важным и влиятельным фактом является то, что данные между оперативной памятью и кэшем передаются блоками, называемыми строкой кэша. Строка кэша имеет фиксированный размер, и в подавляющем большинстве современных компьютеров он составляет 64 байта. Очень важно помнить – вы не можете прочитать или записать меньше данных из памяти, чем размер строки кэша, то есть 64 байта. Даже если вы захотите прочитать один бит из памяти, будет заполнена целая 64-байтовая строка кэша. В этой конструкции используется более быстрый последовательный доступ к DRAM (помните задержки предварительной зарядки и RAS, описанные ранее в этой главе?).</p>

<p class="justify-style">Как уже упоминалось ранее, доступ к DRAM осуществляется с шириной 64 бита (8 байт), поэтому для заполнения такой строки кэша требуется восемь передач из ОЗУ. Это требует многих циклов ЦПУ, поэтому существуют различные техники для оптимизации этого процесса. Одна из них называется "Critical Word First" и "Early Restart". Она позволяет не читать строку кэша слово за словом, а начинать с самого нужного слова. Представьте, что в худшем случае такое 8-байтовое слово может находиться в конце строки кэша, и вам пришлось бы ждать все предыдущие семь передач, чтобы получить доступ к нему. Эта техника сначала читает самое важное слово. Инструкции, ожидающие эти данные, могут продолжить выполнение, а остальная часть строки кэша будет заполнена асинхронно.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Как выглядит типичный шаблон доступа к памяти? Когда кто-то хочет прочитать данные из памяти, соответствующая строка кэша создается в кэше, и в нее считываются 64 байта данных. Когда кто-то хочет записать данные в память, первый шаг точно такой же – строка кэша заполняется в кэше, если ее там еще нет. Эти кэшированные данные изменяются при записи данных. Затем могут произойти две стратегии:</p>
    <ul class="bullet-list ms-1">
      <li>
      <p class="justify-style"><span class="fw-bold fst-italic">Запись через</span>: После записи в строку кэша измененные данные немедленно сохраняются в основной памяти. Это простой подход для реализации, но создает большую нагрузку на шину памяти.</p>
      </li>
      <li>
      <p class="justify-style"><span class="fw-bold fst-italic">Запись обратно</span>: После записи в строку кэша она помечается как грязная. Затем, когда в кэше нет места для других данных, этот грязный блок записывается в память (и измененная грязная запись кэша удаляется). Процессор может записывать эти блоки время от времени, когда сочтет это уместным (например, во время простоя).</p>
      </li>
    </ul>
    <p class="card-text justify-style">Существует еще одна техника оптимизации, называемая объединением записей. Она гарантирует, что данная строка кэша из данной области памяти записывается полностью (а не записываются отдельные слова), снова используя преимущество более быстрого последовательного доступа к памяти.</p>
  </div>
</div>

<p class="justify-style">Из-за строк кэша, данные хранящиеся в памяти, выравниваются по границе в 64 байта. Таким образом, чтобы прочитать два последовательных байта, в худшем случае необходимо использовать две строки кэша общим размером 128 байт. Это показано на <a href="#f-2-8">Рисунке 2-8</a>, когда вы хотите прочитать 2 байта по адресу A, но он находится всего в одном байте до конца границы строки кэша, в таком случае вам в итоге придётся читать две строки кэша.</p>

<figure id="f-2-8" class="figure">
  <img src="content/img/2-8.png" class="img-fluid" alt="Рисунок 2-8" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-8. Доступ к двум последовательным байтам требует заполнения двух строк кэша, поскольку они, к сожалению, были расположены на границе двух сток кеша.</figcaption>
</figure>

<p class="justify-style">Вы можете задаться вопросом, в чем смысл тратить время на такие детали реализации оборудования? Имеет ли это значение в комфортном мире управляемого кода? Давайте выясним.</p>

<p class="justify-style">Стоимость непоследовательных шаблонов доступа к памяти была проиллюстрирована примером кода из <a href="#l-2-1">Листинга 2-1</a> и результатами в <a href="#t-2-1">Таблице 2-1</a>. Примерная программа обращается к одному и тому же двумерному массиву двумя способами – построчно и по столбцам. Результаты представлены для трех различных сред: ПК (Intel Core i7-4770K 3.5GHz), ноутбук (Intel Core i7-4712MQ 2.3GHz) и плата Raspberry Pi 2 (ARM Cortex-A7 0.9GHz).</p>

<br>
<figure id="l-2-1" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      // По строкам
      int[,] tab = new int[n, m];
      for (int i = 0; i &lt; n; ++i)
      {
        for (int j = 0; j &lt; m; ++j)
        {
          tab[i, j] = 1;
        }
      }

      // По столбцам
      int[,] tab = new int[n, m];
      for (int i = 0; i &lt; n; ++i)
      {
        for (int j = 0; j &lt; m; ++j)
        {
          tab[j, i] = 1;
        }
      }
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-1. Индексация по столбцам и строкам при доступе к массиву (массив 5000x5000 целых чисел)</figcaption>
</figure>

<figure id="t-2-1" class="figure">
  <table class="table table-striped table-hover border-1 ">
    <thead>
      <tr>
        <th scope="col">#</th>
        <th scope="col">шаблон</th>
        <th Scope="col">ПК</th>
        <th Scope="col">Ноутбук</th>
        <th Scope="col">Raspberry Pi 2</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th scope="row">1</th>
        <td>По строкам</td>
        <td>52 ms</td>
        <td>127 ms</td>
        <td>918 ms</td>
      </tr>
      <tr>
        <th scope="row">2</th>
        <td>По столбцам</td>
        <td>401 ms</td>
        <td>413 ms</td>
        <td>2001 ms</td>
      </tr>
    </tbody>
  </table>
  <figcaption class="figure-caption">Таблица 2-1. Результаты индексации по столбцам и строкам (n,m = 5000)</figcaption>
</figure>

<p class="justify-style">Этот пример показывает, насколько пагубным для производительности может быть непоследовательное извлечение данных. Пример программы во второй версии считывает данные по столбцам. В результате активная строка ячеек DRAM должна изменяться время от времени. Но что более важно, кэш используется очень неэффективно, потому что только один байт данных считывается при загрузке всей строки кэша. А затем считывается другой удаленный адрес, поэтому необходимо заполнить другую строку кэша. Разница в производительности может быть более чем в семь раз, как видно из <a href="#t-2-1">Таблицы 2-1</a>. ЦПУ часто простаивает, ожидая доступа к памяти.</p>

<p class="justify-style"><a href="#f-2-9">Рисунок 2-9</a> иллюстрирует разницу между доступом к элементам по строкам и по столбцам небольшого массива, содержащего значения от 1 до 40 (и на иллюстрации предполагается, что четыре значения помещаются в одну строку кэша). Предположим также для иллюстративных целей, что у ЦПУ достаточно кэша, чтобы вместить только четыре строки кэша. Когда память считывается построчно (левая сторона <a href="#f-2-9">Рисунка 2-9</a>), последовательные целые числа считываются в пределах последовательных округленных до строки кэша областей памяти:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Чтобы прочитать первые четыре элемента (1,2,3,4), считывается первая строка кэша, и все эти элементы используются.</p>
  </li>
  <li>
    <p class="justify-style">Чтобы прочитать следующие четыре элемента (5,6,7,8), считывается вторая строка кэша, и снова все эти элементы используются.</p>
  </li>
  <li>
    <p class="justify-style">Чтобы прочитать следующие четыре элемента (9,10,11,12), считывается третья строка кэша. Этот доступ повторяется по всему массиву, и использование строк кэша является оптимальным.</p>
  </li>
</ul>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">В реальном ЦП «буфер» для строк кэша представляет собой весь кэш ЦП, поэтому он обычно вмещает сотни или тысячи записей размером со строку кэша шириной 64 байта.</p>
  </div>
</div>

<p class="justify-style">Правая сторона <a href="#f-2-9">Рисунка 2-9</a> показывает второй шаблон, когда одно целое число считывается для каждой строки кэша, а затем переходит к другой:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Чтобы прочитать первые четыре элемента, считываются четыре строки кэша, но используется только один элемент из каждой из них (1 из первой строки кэша, 9 из второй и так далее).</p>
  </li>
  <li>
    <p class="justify-style">Чтобы прочитать следующий элемент (33), одна из уже загруженных строк кэша должна быть очищена, потому что буфер уже заполнен. Скорее всего, это будет наименее используемая строка (содержащая элементы 1,2,3,4) и заменена на новую (содержащую 33,34,35,36).</p>
  </li>
  <li>
    <p class="justify-style">Чтобы прочитать следующий элемент (2), снова будет очищена наименее используемая строка, и процессору потребуется перезагрузить первую строку (содержащую 1,2,3,4), выгруженную только что.</p>
  </li>
  <li>
    <p class="justify-style">Этот шаблон доступа повторяется много раз, требуя считывания строки кэша четыре раза.</p>
  </li>
</ul>

<figure id="f-2-9" class="figure">
  <img src="content/img/2-9.png" class="img-fluid" alt="Рисунок 2-9" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-9. Шаблон доступа по строкам и столбцам – стрелки показывают доступ, вызывающий недействительность строки кэша (при доступе к первым десяти элементам)</figcaption>
</figure>

<p class="justify-style">Очевидно, что реальные ЦПУ имеют больше четырех буферов строк кэша, и строка кэша вмещает больше данных, чем четыре целых значения, поэтому <a href="#f-2-9">Рисунок 2-9</a> является упрощением для иллюстративных целей. Но точно такая же проблема возникает в реальных сценариях, и ее результаты четко видны в <a href="#t-2-1">Таблице 2-1</a>.</p>

<p class="justify-style">Как видите, вся среда выполнения .NET и ее продвинутые методы управления памятью не могут скрыть те детали реализации ЦПУ, которые нас подводят. Неблагоприятный шаблон доступа к памяти вызывает многократное ухудшение производительности вашего кода. Подобный тест для Java и C/C++ даст аналогично неблагоприятные результаты.</p>

<p class="h5 p-2">Выравнивание данных</p>

<p class="justify-style">Существует еще один очень важный аспект доступа к памяти, который необходимо описать. Большинство архитектур ЦПУ спроектированы для доступа к правильно выровненным данным – это означает, что начальный адрес таких данных является кратным заданному выравниванию, указанному в байтах. Каждый тип данных имеет свое собственное выравнивание, и выравнивание структуры данных зависит от выравнивания ее полей. Необходимо уделять много внимания, чтобы не обращаться к невыровненным данным, так как это может быть в несколько раз медленнее. Это ответственность компилятора и разработчика, проектирующего структуры данных. В случае структур данных CLR, компоновка в основном управляется самой средой выполнения. Именно поэтому вы можете заметить много кода в сборщике мусора, связанного с правильной обработкой выравнивания. В Главе 13 вы увидите, как выглядит компоновка памяти объектов и как она может быть контролируема с учетом выравнивания данных.</p>

<p class="justify-style">Однако, начиная с .NET Core 3.0, были введены так называемые аппаратные встроенные функции. Они предоставляют доступ ко многим аппаратно-специфичным инструкциям ЦП, которые не могут быть легко раскрыты с помощью более универсального механизма. Использование выровненной памяти для загрузки и сохранения доступно с тех пор благодаря таким методам, как LoadAlignedVector256 или StoreAligned. Они будут полезны только если вы работаете на очень низком уровне, в основном используя неуправляемую, собственную память через указатели. Они могут быть особенно полезны в так называемых методах векторизации — преобразовании алгоритмов из работы с одним значением на итерацию в работу с набором значений (векторов) на итерацию, с помощью специальных инструкций ЦП SIMD (Single Instruction, Multiple Data).</p>

<p class="h5 p-2">Не временной доступ</p>

<p class="justify-style">До сих пор упоминалось, что в большинстве распространенных типов архитектуры ЦПУ нет доступа к памяти, кроме как через кэш. Вся память, читаемая или записываемая из DRAM процессором, хранится в кэше. Предположим, вы хотите инициализировать очень большой массив, но знаете, что будете использовать его в отдаленном будущем. Из того, что вы узнали до сих пор, вы знаете, что такая инициализация массива вызовет большой трафик памяти. Массив будет записан блоками, одна строка кэша за другой. Более того, каждая из этих операций записи включает три шага – чтение данных в кэш, изменение содержимого кэша и, наконец, запись строки кэша обратно в основную память. В этом сценарии строки кэша заполняются только для записи данных обратно в основную память. Это не только не оптимально само по себе, но и тратит пространство кэша, которое могло бы быть использовано для других программ.</p>

<p class="justify-style">Вы можете избежать такого трафика кэша, используя так называемый набор ассемблерных инструкций не временного доступа – MOVNTI, MOVNTQ, MOVNTDQ и т.д. Они позволяют программисту предотвратить кэширование данных во время записи в память. Они доступны через набор функций C/C++ <code class="fs-6 fst-italic">_mm_stream_*</code>, поэтому для их использования не требуется ассемблер. Например, <code class="fs-6 fst-italic">_mm_stream_si128</code> выполняет инструкцию MOVNTDQ, которая записывает один квадро-слово (4 слова по 4 байта) непосредственно в память. Пример быстрой инициализации массива с использованием этой техники показан в <a href="l-2-2">Листинге 2-2</a>.</p>

<br
<figure id="l-2-2" class="figure">
  <pre class="code border border-secondary">
    <code class="language-cpp">
      #include &lt;emmintrin.h&gt;
      void setbytes(char *p, int c)
      {
        __m128i i = _mm_set_epi8(c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c); // sets 16 
        signed 8-bit integer values
        _mm_stream_si128((__m128i *)&p[0], i);
        _mm_stream_si128((__m128i *)&p[16], i);
        _mm_stream_si128((__m128i *)&p[32], i);
        _mm_stream_si128((__m128i *)&p[48], i);
      }
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-2. Пример использования низкоуровневого API в C++ для не временных записей</figcaption>
</figure>

<p class="justify-style">Упомянутые выше аппаратные встроенные функции доступны с .NET Core 3.0 и также включают возможность использования не временного доступа.</p>

<p class="justify-style">Вы можете использовать набор функций StoreAlignedNonTemporal, которые будут переведены JIT-компилятором в одну из инструкций MOVNTxx, в зависимости от типа данных памяти, к которой вы обращаетесь (байты, целые числа, числа с плавающей запятой и т.д.).</p>

<p class="justify-style">В <a href="#l-2-3">Листинге 2-3</a> вы можете увидеть пример простой программы, которая умножает значения из входного массива на 2 партиями, сохраняя их с помощью вышеупомянутого метода.</p>

<figure id="l-2-3" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      int simdLength = Vector&lt;float&gt;.Count;
      var vec2 = new Vector&lt;float&gt;(2.0f);

      unsafe
      {
        fixed (float* p = outputArray)
        {
          for (; i &lt;= arrayLength - simdLength; i += simdLength)
          {
            var vector = new Vector&lt;float&gt;(inputArray, i);
            vector = vector * vec2;
            vector.StoreAlignedNonTemporal(p + i);
          }
        }
      }
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-3. Пример использования аппаратных встроенных функций для использования не временного доступа</figcaption>
</figure>

<p class="justify-style">Одна сложная вещь, которую нужно помнить, заключается в том, что такие записи должны быть "выровнены". То есть, адреса памяти, к которым вы записываете с помощью StoreAlignedNonTemporal, должны быть кратны размеру строки кэша (32 байта). Это не гарантируется по умолчанию для обычных управляемых массивов float, поэтому вам нужно решить эту проблему двумя возможными способами:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Вместо использования обычного управляемого массива вы можете использовать выровненную нативную память, выделенную с помощью метода NativeMemory.AlignedAlloc (введенного в .NET 6).</p>
    </li>
    <li>
    <p class="justify-style">Вы можете найти первый выровненный адрес в массиве и начать обработку с него. Остальное следует обрабатывать без использования не временного, не выровненного API.</p>
  </li>
</ul>

<p class="justify-style"> В общем, вы должны понимать, что использование не временного доступа - это сложная и хитрая вещь. Это определенно не должно использоваться в качестве стандартной "быстрой техники доступа к памяти".</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Существуют также инструкции загрузки с не временным доступом (nTA) MOVNTDQA, доступные через функции <code class="fs-6 fst-italic">_mm_stream_load_si128</code>. Соответственно, существует набор методов LoadAlignedNonTemporal, доступных через API аппаратных встроенных функций в .NET.</p>
  </div>
</div>

<p class="h5 p-2">Предварительная выборка</p>

<p class="justify-style">Существует еще один механизм, который стремится улучшить использование кэша. Он заключается в заполнении кэша данными, которые, вероятно, понадобятся в ближайшем будущем. Этот механизм называется предварительной выборкой (prefetching), и он может работать в двух разных режимах:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Аппаратно управляемая</span>: Когда ЦПУ замечает несколько промахов кэша с определенными шаблонами. Большинство ЦПУ отслеживают от 8 до 16 шаблонов доступа к памяти (чтобы компенсировать типичную многопоточную/многопроцессный способ работы).</p>
    </li>
    <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Программно управляемая</span>: Через явный вызов инструкции PREFETCHT0, доступной через функцию _mm_prefetch в C/C++.</p>
  </li>
</ul>

<p class="justify-style">Предварительная выборка, как и все другие механизмы кэширования, является обоюдоострым оружием. Если вы хорошо понимаете шаблоны доступа к памяти в вашем коде, то использование предварительной выборки может заметно ускорить производительность вашей программы. С другой стороны, очень сложно быть уверенным, что вы правильно понимаете эти шаблоны доступа к памяти, учитывая очень широкий контекст, в котором работает ваш код – под влиянием других потоков в вашей программе, потоков других программ и потоков самой операционной системы. Время имеет решающее значение: если вы выполните предварительную выборку слишком поздно, данные не будут доступны, когда они вам понадобятся. С другой стороны, если вы выполните предварительную выборку слишком рано, данные могут быть вытеснены из кэша к тому времени, когда вы начнете их использовать. Предварительная выборка используется сборщиком мусора на x86, x64 и ARM64 (см. <a href="#l-2-4">Листинг 2-4</a>).</p>

<figure id="l-2-4" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      // enable on processors known to have a useful prefetch instruction
      #if defined(TARGET_AMD64) || defined(TARGET_X86) || defined(TARGET_ARM64)
      #define PREFETCH
      #endif
      #ifdef PREFETCH
      inline void Prefetch(void* addr)
      {
      #ifdef TARGET_WINDOWS
      #if defined(TARGET_AMD64) || defined(TARGET_X86)
      #ifndef _MM_HINT_T0
      #define _MM_HINT_T0 1
      #endif
        _mm_prefetch((const char*)addr, _MM_HINT_T0);
      #elif defined(TARGET_ARM64)
        __prefetch((const char*)addr);
      #endif //defined(TARGET_AMD64) || defined(TARGET_X86)
      #elif defined(TARGET_UNIX)
        __builtin_prefetch(addr);
      #else //!(TARGET_WINDOWS || TARGET_UNIX)
      UNREFERENCED_PARAMETER(addr);
      #endif //TARGET_WINDOWS
      }
      #else //PREFETCH
      inline void Prefetch (void* addr)
      {
        UNREFERENCED_PARAMETER(addr);
      }
      #endif //PREFETCH
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-4. Части кода .NET, связанные с предварительной выборкой, в зависимости от архитектуры</figcaption>
</figure>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Примером плохого использования кэша может быть ситуация, когда алгоритм сборки мусора спроектирован таким образом, что некоторые очень маленькие, 1-байтовые диагностические данные разбросаны по всей памяти в случайных местах. Операция по сбору этой информации будет очень затратной с точки зрения кэширования. Нам придется заполнять кэш через строки кэша, чтобы прочитать всего один байт.</p>
  </div>
</div>

<p class="justify-style">Алгоритмы, которые интенсивно работают с памятью (а сборка мусора по своей сути работает с памятью), должны учитывать эти внутренние особенности ЦПУ. Память - это не просто плоское пространство, где можно случайным образом выбирать байты здесь и там без каких-либо последствий!</p>

<p class="h5 p-2">Иерархический кэш</p>

<p class="justify-style">Возвращаясь к архитектуре оборудования, из-за требований к производительности, с одной стороны, и оптимизации затрат, с другой, дизайн ЦПУ сегодня эволюционировал в более сложную иерархическую кэш-память. Идея проста. Вместо одного кэша давайте создадим несколько, с разными размерами и скоростями. Это позволяет создать очень маленький и очень быстрый кэш первого уровня (называемый L1), затем немного больший и немного медленнее кэш второго уровня (L2), и, наконец, кэш третьего уровня (L3). В современной архитектуре эта нумерация заканчивается на трех уровнях. Такая иерархическая кэш-память современных компьютеров показана на <a href="#f-2-10">Рисунке 2-10</a>. Правда, иногда можно встретить процессоры, оснащенные кэшем L4, но это немного другой вид памяти, предназначенный в основном для интегрированных графических карт внутри этих ЦПУ.</p>

<figure id="f-2-10" class="figure">
  <img src="content/img/2-10.png" class="img-fluid" alt="Рисунок 2-10" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-10. ЦПУ с иерархическим кэшем – кэш первого уровня разделен на кэш инструкций (L1i) и кэш данных (L1d), а также кэш второго (L2) и третьего (L3) уровней. ЦПУ подключен к ОЗУ через шину памяти</figcaption>
</figure>

<p class="justify-style">Кэш первого уровня разделен на два отдельных блока. Один предназначен для данных (обозначен как L1d), а другой для инструкций (обозначен как L1i). Инструкции, считываемые из памяти и выполняемые процессором, также фактически являются данными, но интерпретируются соответствующим образом. Данные и инструкции кода на уровнях выше L1 фактически обрабатываются одинаково. Однако практика показала, что предпочтительнее обрабатывать данные и инструкции отдельно для самого нижнего уровня кэша. Это подход архитектуры Гарварда. По этой причине архитектура современных компьютеров называется <span class="fw-bold fst-italic">модифицированной Гарвардской архитектурой</span>. Это решение хорошо работает из-за сильной независимости использования областей памяти для хранения данных и программного кода, но только на самом нижнем уровне.</p>

<p class="justify-style">Зная, что существует три основных уровня кэша, возникает очевидный вопрос: каковы типичные различия в скорости и размере между ними и основной памятью? Память на нижних уровнях кэша может быть очень быстрой, настолько, что доступ к L1 и даже L2 может быть быстрее времени выполнения конвейера (если только вам не нужно ждать точного вычисления адреса, что также является дорогостоящей операцией). Итак, каковы эти временные интервалы?</p>

<p class="justify-style">На момент написания этой главы использовался ноутбук с процессором Intel Core i7-4712MQ (поколение Haswell), работающим на частоте 2,30 ГГц. Предполагая, что один цикл процессора на моем ноутбуке занимает примерно 0,4 нс (~1/2,30 ГГц) и используя спецификацию Haswell i7, задержка доступа к различным уровням памяти может быть представлена, как показано в <a href="#t-2-2">Таблице 2-2</a>.</p>

<figure id="t-2-2" class="figure">
  <table class="table table-striped table-hover border-1 ">
    <thead>
      <tr>
        <th scope="col">#</th>
        <th scope="col">Операция</th>
        <th Scope="col">Задержка</th>

            </tr>
          </thead>
          <tbody>
            <tr>
        <th scope="row">1</th>
        <td>Кэш L1</td>
        <td> &lt; 2.0 нс</td>

            </tr>
            <tr>
        <th scope="row">2</th>
        <td>Кэш L2</td>
        <td>4.8 нс</td>
            </tr>
            <tr>
        <th scope="row">3</th>
        <td>Кэш L3</td>
        <td>14.4 нс</td>
            </tr>
            <tr>
        <th scope="row">4</th>
        <td>Основная память</td>
        <td>71.4 нс</td>
            </tr>
            <tr>
        <th scope="row">5</th>
        <td>HDD</td>
        <td>150 000 нс</td>
            </tr>
          </tbody>
        </table>
        <figcaption class="figure-caption">Таблица 2-2. Задержка доступа к различным частям памяти</figcaption>
</figure>

<p class="justify-style">Вы можете ясно видеть, что стоит бороться за оптимальное использование кэша. Задержка может быть в пять раз быстрее, когда необходимые данные доступны в кэше L3, а не в оперативной памяти. С кэшем L1 это более чем в 30 раз лучше. Вот почему чрезвычайно важно для общей производительности знать, как используется кэш. Сколько данных помещается в кэш? Все зависит от конкретной модели процессора, но спецификация i7-4770K довольно хорошо отражает рыночные стандарты. Кэш L1 имеет 64 КБ данных (разделенных на 32 КБ для кода и 32 КБ для данных), в то время как кэш L2 имеет 256 КБ. Кэш L3, всегда намного больше, составляет 8 МБ.</p>

<p class="justify-style">Каково влияние этих временных интервалов в управляемом мире .NET? Давайте рассмотрим простой пример, показывающий задержку при доступе к данным в зависимости от объема обрабатываемой памяти. Мы используем код из <a href="#l-2-5">Листинга 2-5</a>, который выполняет серию последовательных чтений (оптимальный случай). Поскольку используемая структура имеет размер 64 байта, чтение выполняется с шагом 64 байта, и каждый раз необходимо загружать новую строку кэша. На <a href="#f-2-11">Рисунке 2-11</a> показаны средние времена доступа к одному элементу массива tab в зависимости от того, сколько памяти этот массив занимал в целом.</p>

<p class="justify-style">Очевидно ухудшение времени доступа, когда размер данных превышает размер кэша каждого уровня. Поскольку тесты проводились на процессоре Intel i7-4770K, четко видны точки деградации производительности около 256 КБ и 8192 КБ, что соответствует размерам кэша L2 и L3. Вы можете видеть, что работа с небольшими размерами данных может быть в несколько раз быстрее, чем работа с данными, которые не помещаются в кэш L3.</p>

<br>
<figure id="l-2-5" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      public struct OneLineStruct
      {
        public long data1;
        public long data2;
        public long data3;
        public long data4;
        public long data5;
        public long data6;
        public long data7;
        public long data8;
      }

      public static long OneLineStructSequentialReadPattern(OneLineStruct[] tab)
      {
        long sum = 0;
        int n = tab.Length;
        for (int i = 0; i &lt; n; ++i)
        {
          unchecked { sum += tab[i].data1; }
        }
        return sum;
      }
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-5. Последовательное чтение следующих строк кэша</figcaption>
</figure>

<figure id="f-2-11" class="figure">
  <img src="content/img/2-11.png" class="img-fluid" alt="Рисунок 2-11" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-11. Время доступа в зависимости от размера данных – архитектура Intel x86/последовательное чтение. Обратите внимание: обе оси логарифмические</figcaption>
</figure>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Существует одна интересная, но не столь важная тема в контексте кэша – стратегии вытеснения. Речь идет о том, как освободить место для новых данных, если их нет на данном уровне. Существуют два возможных подхода, иногда смешиваемых на разных уровнях:</p>
    <ul class="bullet-list ms-1">
      <li>
        <p class="justify-style"><span class="fw-bold fst-italic">Эксклюзивный кэш</span>: Данные находятся только на одном уровне кэша. Этот метод чаще всего используется в процессорах AMD.</p>
      </li>
      <li>
        <p class="justify-style"><span class="fw-bold fst-italic">Инклюзивный кэш</span>: Когда каждая строка кэша на нижнем уровне (например, L1d) также присутствует на более высоком уровне (например, L2).</p>
      </li>
    </ul>
    <p class="card-text justify-style">Хотя это интересно, но это не влияет на ваше понимание управления памятью. Следует предположить, что производители процессоров делают все возможное, чтобы обеспечить наиболее эффективную реализацию этих механизмов.</p>
  </div>
</div>

<p class="h5 p-2">Многоядерный иерархический кэш</p>

<p class="justify-style">Текст...</p>

<p class="justify-style">Текст...</p>

<hr class="border-2">

<p id="chapter2-2" class="h3 p-2">Операционная система</p>

<p class="justify-style">Текст...</p>

<p class="justify-style">Текст...</p>

<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-2-1" class="h4 p-2">Виртуальная память</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-2-2" class="h4 p-2">Большие страницы</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-2-3" class="h4 p-2">Фрагментация виртуальной памяти</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-2-4" class="h4 p-2">Общая компоновка памяти</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-2-5" class="h4 p-2">Управление памятью в Windows</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-2-6" class="h4 p-2">Компоновка памяти в Windows</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-2-7" class="h4 p-2">Управление памятью в Linux</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-2-8" class="h4 p-2">Компоновка памяти в Linux</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-2-9" class="h4 p-2">Влияние операционной системы</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-3" class="h3 p-2">NUMA и группы ЦПУ</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-4" class="h3 p-2">Резюме</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-4-1" class="h4 p-2">Правило 2 - Следует избегать случайного доступа, поощрять последовательный доступ</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-4-2" class="h4 p-2">Правило 3 - Улучшайте пространственную и временную локальность данных</p>
<p class="justify-style">Текст...</p>

<hr class="border-2">
<p id="chapter2-4-3" class="h4 p-2">Правило 4 - Рассмотрите более продвинутые возможности</p>
<p class="justify-style">Текст...</p>


<!--
Глава 2: Низкоуровневое управление памятью
  Аппаратное обеспечение
    Память
    ЦПУ
  Операционная система
    Виртуальная память
    Большие страницы
    Фрагментация виртуальной памяти
    Общая компоновка памяти
    Управление памятью в Windows
    Компоновка памяти в Windows
    Управление памятью в Linux
    Компоновка памяти в Linux
    Влияние операционной системы
  NUMA и группы ЦПУ
  Резюме
    Правило 2 - Следует избегать случайного доступа, поощрять последовательный доступ
    Правило 3 - Улучшайте пространственную и временную локальность данных
    Правило 4 - Рассмотрите более продвинутые возможности
-->
    
    