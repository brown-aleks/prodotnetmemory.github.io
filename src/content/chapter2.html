<h2 id="chapter2" class="h2 p-2">Глава 2: Низкоуровневое управление памятью</h2>

<hr class="border-2">

<p class="justify-style">В предыдущей главе вы узнали теоретические основы управления памятью. Теперь вы можете сразу перейти к деталям автоматического управления памятью, как работает сборщик мусора и где могут возникать утечки памяти. Но если вы действительно хотите «освоить» эту тему, стоит потратить еще немного времени на низкоуровневые аспекты управления памятью. Это позволит вам лучше понять различные проектные решения, которые были приняты создателями сборщика мусора в .NET (а также других управляемых сред выполнения). Создатели таких механизмов не живут в вакууме и должны адаптироваться к ограничениям и механизмам, которые управляют компьютерным оборудованием и операционными системами.</p>

<p class="justify-style">Вы узнаете о этих механизмах и ограничениях в этой главе. Честно говоря, не так просто представить такие темы в не перегружающей форме. К сожалению, управление памятью в .NET было бы неполным без углубления в эти детали.</p>

<p class="justify-style">Хотя оператор «new» достаточен для большинства сценариев управления памятью в .NET, более глубокое понимание основных процессов и механизмов может оказаться полезным. Оборудование, операционная система и компилятор влияют на то, как это работает и как был написан .NET, хотя это не всегда очевидно. Эти знания очень согласуются с духом Механической Симпатии, представленной в предыдущей главе. Мы надеемся, что вам также будет просто интересно узнать некоторые из упомянутых здесь мелких фактов.</p>

<p class="justify-style">Еще раз, если вы торопитесь или просто хотите перейти к более практическим внутренним аспектам .NET и примерам, не стесняйтесь просмотреть эту главу и вернуться к ней в более свободное время, надеемся.</p>

<hr class="border-2">
<p id="chapter2-1" class="h3 p-2">Аппаратное обеспечение</p>

<p class="justify-style"> Как работает современный компьютер? Вы, вероятно, уже имеете базовое представление о предмете: компьютер состоит из процессора, который является основной вычислительной единицей – он выполняет программы. У него есть доступ к оперативной памяти (которая быстрая) и жестким дискам (которые медленные). Также есть видеокарта, которая очень важна для геймеров (и различных видов графических дизайнеров), которая отвечает за создание изображения, отображаемого на мониторе. Такой обзор с высоты птичьего полета недостаточен для наших целей. Давайте углубимся в тему. Для целей ваших размышлений давайте введем архитектуру современного компьютера, как на диаграмме на <a href="#f-2-1">Рисунке 2-1</a>.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Современный рынок персональных компьютеров доминируют ПК и Маки. Смоделированная схема архитектуры общего компьютера основана на них. При необходимости будут введены некоторые возможные нюансы, такие как те, которые касаются процессоров ARM или более сложных серверных машин.</p>
  </div>
</div>

<p class="justify-style">Основные компоненты типичной архитектуры компьютера можно перечислить как</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Процессор</span> (CPU, центральный процессор): Основной блок, отвечающий за выполнение инструкций, как описано в Главе 1. Здесь находятся такие компоненты, как арифметико-логические устройства (ALU), устройства с плавающей запятой (FPU), регистры и конвейеры выполнения инструкций, которые делят инструкции на набор более мелких операций и выполняют их, если возможно, параллельно.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Шина передней панели</span> (FSB): Шина данных, соединяющая процессор с северным мостом.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Северный мост</span>: Блок, содержащий в основном контроллер памяти, отвечающий за управление связью между памятью и процессором.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">ОЗУ</span> (оперативная память): Основная память компьютера. Она хранит данные и код программ до тех пор, пока питание включено – поэтому ее также называют динамической оперативной памятью (DRAM) или энергозависимой памятью.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Шина памяти</span>: Шина данных, соединяющая ОЗУ с северным мостом.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Южный мост</span>: Чип, который обрабатывает все функции ввода-вывода, такие как USB, аудио, последовательный порт, системный BIOS, шина ISA, контроллер прерываний и каналы IDE – контроллеры массового хранения, такие как PATA и/или SATA.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Ввод-вывод хранения</span>: Энергонезависимая память, которая хранит данные, включая популярные HDD или SSD диски.</p>
  </li>
</ul>

<figure id="f-2-1" class="figure">
  <img src="content/img/2-1.png" class="img-fluid" alt="Рисунок 2-1" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-1. Архитектура компьютера – ЦПУ, ОЗУ, северный мост, южный мост и другие. Ширина шины иллюстрирует пропорцию объема передаваемых данных (очень приблизительно)</figcaption>
</figure>

<p class="justify-style">Стоит упомянуть, что ранее процессор, северный мост и южный мост были отдельными чипами, но теперь они тесно интегрированы. Начиная с микроархитектур Intel Nehalem и AMD Zen, северный мост включен в кристалл процессора (который в таком случае часто называют <span class="fw-bold fst-italic">uncore</span> или <span class="fw-bold fst-italic">System Agent</span>). Эта эволюция архитектуры показана на <a href="#f-2-2">Рисунке 2-2</a>.</p>

<figure id="f-2-2" class="figure">
  <img src="content/img/2-2.png" class="img-fluid" alt="Рисунок 2-2" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-2. Современное оборудование – процессор с северным мостом внутри, ОЗУ, южный мост (переименованный в Platform Controller Hub в случае терминологии Intel) и другие. Ширина шины иллюстрирует пропорцию объема передаваемых данных (очень приблизительно)</figcaption>
</figure>

<p class="justify-style">Такая интеграция помогает, потому что контроллер памяти (внутри северного моста) расположен ближе к исполнительным блокам процессора, уменьшая задержки за счет меньших физических расстояний и улучшенного взаимодействия. Но на рынке все еще есть процессоры (наиболее популярные из которых – семейство AMD FX), у которых процессор, северный мост и южный мост разделены.</p>

<p class="justify-style">Основная проблема любого управления памятью заключается в несоответствии производительности современных процессоров по отношению к подсистемам памяти и массового хранения. Процессор намного быстрее памяти, поэтому каждый доступ к памяти вызывает нежелательные задержки. Когда процессору приходится ждать доступа к памяти (чтение или запись), это называется остановкой. Остановки негативно влияют на использование процессора, так как приводят к потере циклов процессора на ожидание, а не на выполнение задач.</p>

<p class="justify-style">Типичный современный процессор работает на частоте 3 ГГц или выше. Между тем, память работает с внутренними тактовыми частотами другого порядка величины, всего 200–400 МГц. Было бы слишком дорого создавать микросхемы ОЗУ, работающие на частоте процессоров. Это связано с тем, как устроены современные ОЗУ – зарядка и разрядка внутренних конденсаторов занимает время и его очень трудно уменьшить.</p>

<p class="justify-style">Вас может удивить, что память работает с такими низкими частотами. На самом деле, в компьютерных магазинах модули памяти рекламируются с частотами, такими как 3200 или 4800 МГц, которые гораздо ближе к скорости процессора. Откуда берутся такие цифры? Как вы увидите, такие спецификации – это только часть более сложной истины.</p>

<p class="justify-style">Модули памяти состоят из <span class="fw-bold fst-italic">внутренних ячеек памяти</span> (хранящих данные) и дополнительных буферов, которые помогают преодолеть их низкие внутренние тактовые частоты. Используются некоторые дополнительные приемы (см. <a href="#f-2-3">Рисунок 2-3</a>). Большинство из них основаны на умножении чтения данных:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Отправка данных из внутренней ячейки памяти дважды в течение одного тактового цикла. Для точности, это как на спаде, так и на подъеме сигнала. Отсюда и название самой популярной памяти различных поколений – <span class="fw-bold fst-italic">Double Data Rate</span> (DDR). Этот метод также называют <span class="fw-bold fst-italic">двойной накачкой</span>.</p>
  </li>
  <li>
    <p class="justify-style">Использование внутренней буферизации для выполнения нескольких чтений одновременно («режим burst») в одном тактовом цикле памяти. Это умножает количество прочитанных данных при той же внутренней частоте. Интерфейс памяти DDR2 удваивает внешнюю тактовую частоту, в то время как DDR3 и DDR4 увеличивают ее в четыре раза. DDR5 удваивает ее еще раз.</p>
  </li>
</ul>

<p class="justify-style">Эти методы в настоящее время используются в модулях DDR в отличие от гораздо более простых модулей SDRAM (синхронный DRAM), использовавшихся в прошлом. В конечном итоге, в случае современных типичных DDR5, кратность «burst» модуля памяти составляет 16, поскольку он сочетает технику двойной накачки с восемью чтениями одновременно.</p>

<figure id="f-2-3" class="figure">
  <img src="content/img/2-3.png" class="img-fluid" alt="Рисунок 2-3" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-3. Внутреннее устройство SDRAM, DDR, DDR2, DDR3, DDR4 и DDR5. Пример модулей памяти с внутренней частотой 300 МГц. MT/s означает «Мега передача в секунду». Обратите внимание, что это не строгая, а скорее иллюстративная диаграмма, показывающая соотношения между внутренними частотами и результирующими MT/s</figcaption>
</figure>

<p class="justify-style">Для иллюстрации давайте рассмотрим типичный чип памяти DDR4, например, 16 ГБ 2400 МГц (описанный в спецификациях как DDR4-2400, PC4-19200). В этих случаях внутренняя тактовая частота массива DRAM составляет 300 МГц. Тактовая частота шины памяти увеличивается в четыре раза до 1200 МГц благодаря внутреннему буферу ввода-вывода. Кроме того, происходит две передачи за каждый тактовый цикл (оба склона сигнала), что приводит к скорости передачи данных 2400 MT/s (мега передача в секунду). Отсюда и берется спецификация 2400 МГц. Проще говоря, из-за природы двойной накачки в памяти DDR, скорость обычно указывается как двойная частота тактовой шины ввода-вывода, которая сама по себе является умножением внутренней тактовой частоты памяти. Указание этого значения в МГц – это просто маркетинговое упрощение. Вторая подпись – PC4-19200 – обладает максимальной теоретической производительностью такой памяти – это 2400 МТ/с, умноженные на 8 байт (передается одно слово длиной 64 бита), что дает результат 19200 МБ/с.</p>

<p class="justify-style">Давайте рассмотрим настольный ПК Конрада в контексте всей архитектуры. Он оснащен процессором Intel Core i7-4770K (поколение Haswell), работающим на частоте 3,5 ГГц. Частота шины передней панели составляет всего 100 МГц. Используемая память DDR3-1600 (PC3-12800) имеет внутреннюю тактовую частоту памяти 200 МГц, и благодаря механизму DDR3 тактовая частота шины ввода-вывода составляет 800 МГц. Это показано на <a href="#f-2-4">Рисунке 2-4</a>. Это подтверждается использованием инструментов аппаратной диагностики, таких как CPU-Z (см. <a href="#f-2-5">Рисунок 2-5</a>).</p>

<p class="justify-style">Модули памяти постоянно улучшаются. Например, для DDR5 основным драйвером изменений было улучшение пропускной способности памяти. Вот почему была введена удвоенная длина burst, наряду с другими аналогичными изменениями, такими как удвоение количества «банков» и «групп банков» или введение двух независимых каналов вместо одного. Однако объяснение этих техник потребовало бы объяснения низкоуровневой работы модулей памяти, что выходит за рамки этой книги.</p>

<p class="justify-style">Если вам это интересно, вы можете начать с постера RAM Anatomy, доступного на сайте <a href="https://prodotnetmemory.com/">https://prodotnetmemory.com/</a>.</p>

<figure id="f-2-4" class="figure">
  <img src="content/img/2-4.png" class="img-fluid" alt="Рисунок 2-4" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-4. Современная аппаратная архитектура с дискретной тактовой частотой (Intel Core i7-4770K и DDR3-1600)</figcaption>
</figure>

<figure id="f-2-5" class="figure">
  <img src="content/img/2-5.png" class="img-fluid" alt="Рисунок 2-5" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-5. Скриншот CPU-Z – вкладка «Память», на которой показаны частоты северного моста (NB) и DRAM, а также соотношение частот FSB:DRAM (которое, к сожалению, неверно в данной версии инструмента и должно быть 1:8)</figcaption>
</figure>

<p class="justify-style">Несмотря на все описанные здесь улучшения памяти DDR, процессоры все еще намного быстрее памяти, которую они используют. Чтобы преодолеть эту проблему, применяется аналогичный подход на разных уровнях – приближение части данных к компоненту с более производительными (и более дорогими) блоками памяти. Такой подход называется кэшированием.</p>

<p class="justify-style">Для массовой памяти, такой как HDD, данные обычно кэшируются в ОЗУ – или в более быстрой, но меньшей по размеру выделенной памяти, такой как небольшой SSD внутри гибридных HDD-дисков, предназначенных для наиболее часто используемых данных. Для ОЗУ данные кэшируются внутри кэша процессора, как вы скоро увидите.</p>

<p class="justify-style">Конечно, существуют более общие оптимизации ОЗУ, включая лучшее аппаратное проектирование, лучшие контроллеры памяти и оптимизацию DMA (Direct Memory Access - прямой доступ к памяти) для устройств. Однако DMA не рассматривается в этой книге, так как он не связан напрямую с данными программы, и эти области памяти не управляются сборщиком мусора.</p>

<hr class="border-2">

<p id="chapter2-1-1" class="h4 p-2">Память</p>

<p class="justify-style">В настоящее время существует два основных типа памяти, используемых в персональных компьютерах, которые значительно различаются как по стоимости производства и использования, так и по производительности:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Статическая оперативная память</span> (SRAM): Обеспечивает очень быстрый доступ, но является довольно сложной, состоящей из 4–6 транзисторов на ячейку (хранящую один бит). Она сохраняет данные, пока питание включено, и не требует обновления. Из-за высокой скорости используется в основном в кэшах процессора.</p>
    </li>
    <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Динамическая оперативная память</span> (DRAM): Очень простая конструкция ячейки (гораздо меньше, чем у SRAM) состоит из одного транзистора и конденсатора. Из-за утечки заряда конденсатора ячейка требует постоянного обновления (что занимает драгоценные миллисекунды и замедляет чтение памяти). Сигнал, считанный с конденсатора, должен быть усилен, что усложняет процесс. Чтение и запись также занимают время и не являются линейными из-за задержек конденсатора (требуется некоторое время для получения правильного чтения или успешной записи).</p>
    </li>
</ul>

<p class="justify-style">Давайте уделим еще несколько слов технологии DRAM, так как она является основой широко используемой памяти, установленной в слотах DIMM наших компьютеров. Как уже упоминалось, одна ячейка DRAM состоит из транзистора и конденсатора и хранит один бит данных. Такие ячейки сгруппированы в массивы DRAM. Адрес для доступа к конкретной ячейке предоставляется через так называемые адресные линии.</p>

<p class="justify-style">Было бы очень сложно и дорого, если бы каждая ячейка в массиве DRAM имела свой собственный адрес. Например, в случае 32-битной адресации потребовался бы 32-битный декодер адресных линий (компонент, отвечающий за выбор конкретной ячейки). Количество адресных линий в значительной степени влияет на общую стоимость системы – чем больше линий, тем больше выводов и соединений между контроллером памяти и чипами памяти (модулями). Из-за этого адресные линии используются повторно как строки и столбцы (см. <a href="#f-2-6">Рисунок 2-6</a>), и для предоставления полного адреса требуется дважды записывать на одни и те же линии.</p>

<figure id="f-2-6" class="figure">
  <img src="content/img/2-6.png" class="img-fluid" alt="Рисунок 2-6" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-6. Пример чипа DRAM с массивом DRAM и наиболее важными каналами: адресные линии, 
    RAS и CAS</figcaption>
</figure>

<p class="justify-style">Чтение одного бита из конкретной ячейки занимает несколько шагов:</p>

<ol class="number-list ms-1">
  <li>
    <p class="justify-style">Номер строки помещается на адресные линии.</p>
    </li>
    <li>
    <p class="justify-style">Интерпретация запускается сигналом стробирования адреса строки (RAS) на выделенной линии.</p>
    </li>
    <li>
    <p class="justify-style">Номер столбца помещается на адресные линии.</p>
    </li>
    <li>
      <p class="justify-style">Интерпретация запускается сигналом стробирования адреса столбца (CAS).</p>
    </li>
    <li>
      <p class="justify-style">Строка и столбец указывают на конкретную ячейку DRAM в массиве. Один бит считывается из ячейки и записывается на линию данных.</p>
    </li>
</ol>

<p class="justify-style">Модули DRAM, установленные в наших компьютерах, состоят из множества таких массивов DRAM, организованных таким образом, чтобы мы могли получить доступ к нескольким битам (одному слову) за один тактовый цикл.</p>

<p class="justify-style">Временные интервалы перехода между отдельными шагами получения этого одного бита сильно влияют на производительность памяти. Эти временные интервалы могут быть вам знакомы, так как они являются важным фактором в спецификации модулей памяти, что сильно влияет на их цену. Вы, вероятно, знаете о таймингах модулей DIMM, таких как DDR3 9-9-9-24. Все эти тайминги указывают количество тактовых циклов, необходимых для выполнения определенных действий. Соответственно, они имеют следующие значения:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">tCL (CAS латентность): Время между стробом адреса столбца (CAS) и началом ответа (получением данных).</p>
    </li>
    <li>
    <p class="justify-style">tRCD (задержка RAS до CAS): Минимальное время между стробом адреса строки (RAS) и стробом адреса столбца (CAS).</p>
    </li>
    <li>
    <p class="justify-style">tRP (предзарядка строки): Время, необходимое для предзарядки строки перед доступом к ней. Строка не может быть использована без предварительной подготовки, называемой предзарядкой.</p>
    </li>
    <li>
    <p class="justify-style">tRAS (задержка активной строки): Минимальное время, в течение которого строка должна быть активной для доступа к информации в ней.</p>
    </li>
</ul>

<p class="justify-style">Обратите внимание на важность этих временных интервалов. Если строка и столбец, которые вас интересуют, уже установлены, считывание происходит почти мгновенно. Если вы хотите изменить столбец, это займет tCL тактовых циклов. Если вы хотите изменить строку, ситуация намного хуже: сначала она должна быть перезаряжена (tRP циклы), затем следуют задержки RAS и CAS (tCL и tRCD).</p>

<p class="justify-style">Все эти временные интервалы важны для пользователей компьютеров, ожидающих максимальной производительности. Игроки особенно обращают внимание на эти параметры. При покупке модулей памяти вы должны стремиться к минимально возможным таймингам, которые вы можете себе позволить, если производительность является вашим приоритетом.</p>

<p class="justify-style">Однако нас интересует влияние архитектуры памяти DRAM и ее таймингов на управление памятью. Стоимость изменения строки – временные интервалы сигнала RAS и перезарядка – значительна. Это одна из многих причин, почему последовательные шаблоны доступа к памяти намного быстрее, чем непоследовательные. Чтение данных в режиме burst из одной строки (изменяя столбец время от времени) намного быстрее, чем частое изменение строки. Если шаблон доступа полностью случайный, вы, скорее всего, столкнетесь с этими временными интервалами изменения строки при каждом доступе к памяти.</p>

<p class="justify-style">Вся представленная здесь информация имеет одну цель – убедиться, что у вас есть глубокая причина запомнить, почему непоследовательный доступ к памяти так нежелателен. И, как вы увидите, это не единственная причина, почему полностью случайный доступ является наихудшим сценарием.</p>

<hr class="border-2">

<p id="chapter2-1-2" class="h4 p-2">ЦПУ</p>

<p class="justify-style">Теперь перейдем к теме центрального процессора. Процессор совместим с так называемой архитектурой набора инструкций (ISA) – она определяет, среди прочего, набор операций, которые могут выполняться (инструкции), регистры и их значение, как адресуется память и так далее. В этом смысле ISA является контрактом (интерфейсом), установленным между производителем процессора и его пользователями – программами, написанными в соответствии с данным контрактом. Это уровень, который вы видите при программировании, например, на языке ассемблера данной архитектуры. ISA IA-32 (32-битные процессоры i386, Pentium 32-битные процессоры), совместимые с AMD64 (большинство современных процессоров, включая Intel Core, AMD FX и Zen и т.д.), и A64 для ARM64 являются наиболее широко используемыми в мире экосистемы .NET. Под ISA находится так называемая микроархитектура процессора, которая ее реализует. Это позволяет улучшать микроархитектуру без влияния на систему и программное обеспечение, сохраняя обратную совместимость.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Существует много путаницы с названиями стандартов 64-битной архитектуры, и вы часто можете встретить x86-64, EMT64T, Intel 64 или AMD64, используемые взаимозаменяемо. Несмотря на наличие имен производителей и иногда незначительные различия, для целей этой книги вы можете смело считать, что эти названия однозначны и могут быть безопасно заменены друг на друга.</p>
  </div>
</div>

<p class="justify-style">Как было сказано в предыдущей главе, регистры являются ключевыми компонентами ЦПУ, потому что в настоящее время все компьютеры реализованы как регистровые машины. В контексте манипуляции данными доступ к регистрам является мгновенным в том смысле, что он происходит в течение одного процессорного цикла и не вызывает дополнительных задержек. Нет места для ваших данных ближе к ЦПУ, чем регистры процессора. Конечно, регистры хранят только данные, необходимые для текущих инструкций, поэтому их нельзя считать универсальной памятью. На самом деле, в общем, процессоры имеют больше регистров, чем это видно из их ISA. Это позволяет выполнять различные типы оптимизаций (например, так называемое переименование регистров). Однако это детали реализации микроархитектуры и не влияют на механизмы управления памятью.</p>

<p class="h5 p-2">Кэш ЦПУ</p>

<p class="justify-style">Как мы уже упоминали ранее, чтобы уменьшить разрыв в производительности между ЦПУ и ОЗУ, используется 
  промежуточный компонент для хранения копий наиболее часто используемых и необходимых данных – кэш ЦПУ. В общем виде это 
  показано на <a href="#2-7">Рисунке 2-7</a>.</p>

<figure id="f-2-7" class="figure">
  <img src="content/img/2-7.png" class="img-fluid" alt="Рисунок 2-7" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-7. Взаимосвязь ЦПУ с кэшем и ОЗУ</figcaption>
</figure>

<p class="justify-style">Этот кэш прозрачен с точки зрения ISA. Ни программисту, ни операционной системе не нужно знать о ее существовании. Они не обязаны им управлять. В идеальном мире правильное использование и управление кэшем должно быть исключительной ответственностью центрального процессора.</p>

<p class="justify-style">Поскольку кэш должен быть максимально быстрым, используются ранее упомянутые чипы SRAM. Из-за своей стоимости и размера (занимающего драгоценное место в процессоре) они не могут иметь такую ​​же большую емкость, как основная оперативная память. Но в зависимости от предполагаемых затрат они могут быть такими же быстрыми, как ЦП, или может быть, только на один-два порядка медленнее.</p>

<p class="h5 p-2">Попадание и промах кэша</p>

<p class="justify-style">Идея кэша тривиальна. Когда выполняемая процессором инструкция нуждается в доступе к памяти (будь то запись или чтение), она сначала проверяет кэш, чтобы узнать, находятся ли нужные данные уже там. Если да, то отлично! Вы только что получили очень быстрый доступ к памяти, и такая ситуация называется попаданием в кэш. Если данных нет в кэше (так называемый промах кэша), то сначала их нужно прочитать из ОЗУ перед тем, как сохранить в кэш, что, очевидно, является гораздо более медленной операцией. Соотношение попаданий и промахов кэша являются очень важными показателями, показывающими, насколько эффективно наш код использует кэш.</p>

<p class="h5 p-2">Локальность данных</p>

<p class="justify-style"> Но почему такой кэш вообще полезен? Кэширование основано на очень важной концепции – <span class="fw-bold fst-italic">локальности данных</span>. Вы можете различить два вида локальности:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Временная локальность</span>: Если вы обращаетесь к какому-то региону памяти, вы, скорее всего, обратитесь к нему снова в ближайшем будущем. Это делает использование кэша вполне оправданным – вы читаете некоторые данные из памяти и, вероятно, будете использовать их позже еще несколько раз. В общем, вы загружаете некоторые структуры данных в переменные и используете эти переменные многократно (счетчики, временные данные, считанные из файлов и так далее).</p>
    </li>
    <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Пространственная локальность</span>: Если вы обращаетесь к какому-то региону памяти, вы, скорее всего, обратитесь к данным из близкого окружения. Этот тип локальности может стать вашим союзником, если вы кэшируете немного больше окружающих данных, чем вам нужно в данный момент. Например, если вам нужно несколько байт из памяти, давайте прочитаем и за кэшируем еще десяток байт. Вы редко используете очень изолированные области памяти. Вы скоро обнаружите, что стек и куча организованы таким образом, что потоки, выполняющие свою работу, обычно обращаются к похожим областям памяти. Локальные переменные или поля в структурах данных также обычно размещаются близко друг к другу.</p>
    </li>
</ul>

<p class="justify-style">Обратите внимание, что кэш полезен, если вышеупомянутые условия действительно выполняются. Однако это палка о двух концах. Если вы напишете программу таким образом, что она нарушает локальность данных, кэш станет ненужной обузой. Вы увидите это позже в главе.</p>

<p class="h5 p-2">Реализация кэша</p>

<p class="justify-style">До тех пор, пока сохраняется совместимость с моделью памяти ISA, детали реализации кэша теоретически не имеют значения. Он должен быть просто для ускорения доступа к памяти и все. Тем не менее, это прекрасный пример <span class="fw-bold fst-italic">Закона дырявых абстракций</span>, придуманного Джоэлом Спольски:</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Цитата</div>
  <div class="card-body">
    <p class="card-text justify-style">Все нетривиальные абстракции, в той или иной степени, дырявые</p>
  </div>
</div>

<p class="justify-style">Это означает, что абстракция, которая теоретически должна скрывать детали реализации, к сожалению, при определенных обстоятельствах раскрывает их наружу. И обычно это происходит непредсказуемым и/или нежелательным образом. Как это работает в случае с кэшем, должно стать ясно в ближайшее время, а пока давайте просто немного углубимся в детали реализации.</p>

<p class="justify-style">Самым важным и влиятельным фактом является то, что данные между оперативной памятью и кэшем передаются блоками, называемыми строкой кэша. Строка кэша имеет фиксированный размер, и в подавляющем большинстве современных компьютеров он составляет 64 байта. Очень важно помнить – вы не можете прочитать или записать меньше данных из памяти, чем размер строки кэша, то есть 64 байта. Даже если вы захотите прочитать один бит из памяти, будет заполнена целая 64-байтовая строка кэша. В этой конструкции используется более быстрый последовательный доступ к DRAM (помните задержки предварительной зарядки и RAS, описанные ранее в этой главе?).</p>

<p class="justify-style">Как уже упоминалось ранее, доступ к DRAM осуществляется с шириной 64 бита (8 байт), поэтому для заполнения такой строки кэша требуется восемь передач из ОЗУ. Это требует многих циклов ЦПУ, поэтому существуют различные техники для оптимизации этого процесса. Одна из них называется "Critical Word First" и "Early Restart". Она позволяет не читать строку кэша слово за словом, а начинать с самого нужного слова. Представьте, что в худшем случае такое 8-байтовое слово может находиться в конце строки кэша, и вам пришлось бы ждать все предыдущие семь передач, чтобы получить доступ к нему. Эта техника сначала читает самое важное слово. Инструкции, ожидающие эти данные, могут продолжить выполнение, а остальная часть строки кэша будет заполнена асинхронно.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Как выглядит типичный шаблон доступа к памяти? Когда кто-то хочет прочитать данные из памяти, соответствующая строка кэша создается в кэше, и в нее считываются 64 байта данных. Когда кто-то хочет записать данные в память, первый шаг точно такой же – строка кэша заполняется в кэше, если ее там еще нет. Эти кэшированные данные изменяются при записи данных. Затем могут произойти две стратегии:</p>
    <ul class="bullet-list ms-1">
      <li>
      <p class="justify-style"><span class="fw-bold fst-italic">Запись через</span>: После записи в строку кэша измененные данные немедленно сохраняются в основной памяти. Это простой подход для реализации, но создает большую нагрузку на шину памяти.</p>
      </li>
      <li>
      <p class="justify-style"><span class="fw-bold fst-italic">Запись обратно</span>: После записи в строку кэша она помечается как грязная. Затем, когда в кэше нет места для других данных, этот грязный блок записывается в память (и измененная грязная запись кэша удаляется). Процессор может записывать эти блоки время от времени, когда сочтет это уместным (например, во время простоя).</p>
      </li>
    </ul>
    <p class="card-text justify-style">Существует еще одна техника оптимизации, называемая объединением записей. Она гарантирует, что данная строка кэша из данной области памяти записывается полностью (а не записываются отдельные слова), снова используя преимущество более быстрого последовательного доступа к памяти.</p>
  </div>
</div>

<p class="justify-style">Из-за строк кэша, данные хранящиеся в памяти, выравниваются по границе в 64 байта. Таким образом, чтобы прочитать два последовательных байта, в худшем случае необходимо использовать две строки кэша общим размером 128 байт. Это показано на <a href="#f-2-8">Рисунке 2-8</a>, когда вы хотите прочитать 2 байта по адресу A, но он находится всего в одном байте до конца границы строки кэша, в таком случае вам в итоге придётся читать две строки кэша.</p>

<figure id="f-2-8" class="figure">
  <img src="content/img/2-8.png" class="img-fluid" alt="Рисунок 2-8" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-8. Доступ к двум последовательным байтам требует заполнения двух строк кэша, поскольку они, к сожалению, были расположены на границе двух сток кеша.</figcaption>
</figure>

<p class="justify-style">Вы можете задаться вопросом, в чем смысл тратить время на такие детали реализации оборудования? Имеет ли это значение в комфортном мире управляемого кода? Давайте выясним.</p>

<p class="justify-style">Стоимость непоследовательных шаблонов доступа к памяти была проиллюстрирована примером кода из <a href="#l-2-1">Листинга 2-1</a> и результатами в <a href="#t-2-1">Таблице 2-1</a>. Примерная программа обращается к одному и тому же двумерному массиву двумя способами – построчно и по столбцам. Результаты представлены для трех различных сред: ПК (Intel Core i7-4770K 3.5GHz), ноутбук (Intel Core i7-4712MQ 2.3GHz) и плата Raspberry Pi 2 (ARM Cortex-A7 0.9GHz).</p>

<br>
<figure id="l-2-1" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      // По строкам
      int[,] tab = new int[n, m];
      for (int i = 0; i &lt; n; ++i)
      {
        for (int j = 0; j &lt; m; ++j)
        {
          tab[i, j] = 1;
        }
      }

      // По столбцам
      int[,] tab = new int[n, m];
      for (int i = 0; i &lt; n; ++i)
      {
        for (int j = 0; j &lt; m; ++j)
        {
          tab[j, i] = 1;
        }
      }
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-1. Индексация по столбцам и строкам при доступе к массиву (массив 5000x5000 целых чисел)</figcaption>
</figure>

<figure id="t-2-1" class="figure">
  <table class="table table-striped table-hover border-1 ">
    <thead>
      <tr>
        <th scope="col">#</th>
        <th scope="col">шаблон</th>
        <th Scope="col">ПК</th>
        <th Scope="col">Ноутбук</th>
        <th Scope="col">Raspberry Pi 2</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th scope="row">1</th>
        <td>По строкам</td>
        <td>52 ms</td>
        <td>127 ms</td>
        <td>918 ms</td>
      </tr>
      <tr>
        <th scope="row">2</th>
        <td>По столбцам</td>
        <td>401 ms</td>
        <td>413 ms</td>
        <td>2001 ms</td>
      </tr>
    </tbody>
  </table>
  <figcaption class="figure-caption">Таблица 2-1. Результаты индексации по столбцам и строкам (n,m = 5000)</figcaption>
</figure>

<p class="justify-style">Этот пример показывает, насколько пагубным для производительности может быть непоследовательное извлечение данных. Пример программы во второй версии считывает данные по столбцам. В результате активная строка ячеек DRAM должна изменяться время от времени. Но что более важно, кэш используется очень неэффективно, потому что только один байт данных считывается при загрузке всей строки кэша. А затем считывается другой удаленный адрес, поэтому необходимо заполнить другую строку кэша. Разница в производительности может быть более чем в семь раз, как видно из <a href="#t-2-1">Таблицы 2-1</a>. ЦПУ часто простаивает, ожидая доступа к памяти.</p>

<p class="justify-style"><a href="#f-2-9">Рисунок 2-9</a> иллюстрирует разницу между доступом к элементам по строкам и по столбцам небольшого массива, содержащего значения от 1 до 40 (и на иллюстрации предполагается, что четыре значения помещаются в одну строку кэша). Предположим также для иллюстративных целей, что у ЦПУ достаточно кэша, чтобы вместить только четыре строки кэша. Когда память считывается построчно (левая сторона <a href="#f-2-9">Рисунка 2-9</a>), последовательные целые числа считываются в пределах последовательных округленных до строки кэша областей памяти:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Чтобы прочитать первые четыре элемента (1,2,3,4), считывается первая строка кэша, и все эти элементы используются.</p>
  </li>
  <li>
    <p class="justify-style">Чтобы прочитать следующие четыре элемента (5,6,7,8), считывается вторая строка кэша, и снова все эти элементы используются.</p>
  </li>
  <li>
    <p class="justify-style">Чтобы прочитать следующие четыре элемента (9,10,11,12), считывается третья строка кэша. Этот доступ повторяется по всему массиву, и использование строк кэша является оптимальным.</p>
  </li>
</ul>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">В реальном ЦП «буфер» для строк кэша представляет собой весь кэш ЦП, поэтому он обычно вмещает сотни или тысячи записей размером со строку кэша шириной 64 байта.</p>
  </div>
</div>

<p class="justify-style">Правая сторона <a href="#f-2-9">Рисунка 2-9</a> показывает второй шаблон, когда одно целое число считывается для каждой строки кэша, а затем переходит к другой:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Чтобы прочитать первые четыре элемента, считываются четыре строки кэша, но используется только один элемент из каждой из них (1 из первой строки кэша, 9 из второй и так далее).</p>
  </li>
  <li>
    <p class="justify-style">Чтобы прочитать следующий элемент (33), одна из уже загруженных строк кэша должна быть очищена, потому что буфер уже заполнен. Скорее всего, это будет наименее используемая строка (содержащая элементы 1,2,3,4) и заменена на новую (содержащую 33,34,35,36).</p>
  </li>
  <li>
    <p class="justify-style">Чтобы прочитать следующий элемент (2), снова будет очищена наименее используемая строка, и процессору потребуется перезагрузить первую строку (содержащую 1,2,3,4), выгруженную только что.</p>
  </li>
  <li>
    <p class="justify-style">Этот шаблон доступа повторяется много раз, требуя считывания строки кэша четыре раза.</p>
  </li>
</ul>

<figure id="f-2-9" class="figure">
  <img src="content/img/2-9.png" class="img-fluid" alt="Рисунок 2-9" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-9. Шаблон доступа по строкам и столбцам – стрелки показывают доступ, вызывающий недействительность строки кэша (при доступе к первым десяти элементам)</figcaption>
</figure>

<p class="justify-style">Очевидно, что реальные ЦПУ имеют больше четырех буферов строк кэша, и строка кэша вмещает больше данных, чем четыре целых значения, поэтому <a href="#f-2-9">Рисунок 2-9</a> является упрощением для иллюстративных целей. Но точно такая же проблема возникает в реальных сценариях, и ее результаты четко видны в <a href="#t-2-1">Таблице 2-1</a>.</p>

<p class="justify-style">Как видите, вся среда выполнения .NET и ее продвинутые методы управления памятью не могут скрыть те детали реализации ЦПУ, которые нас подводят. Неблагоприятный шаблон доступа к памяти вызывает многократное ухудшение производительности вашего кода. Подобный тест для Java и C/C++ даст аналогично неблагоприятные результаты.</p>

<p class="h5 p-2">Выравнивание данных</p>

<p class="justify-style">Существует еще один очень важный аспект доступа к памяти, который необходимо описать. Большинство архитектур ЦПУ спроектированы для доступа к правильно выровненным данным – это означает, что начальный адрес таких данных является кратным заданному выравниванию, указанному в байтах. Каждый тип данных имеет свое собственное выравнивание, и выравнивание структуры данных зависит от выравнивания ее полей. Необходимо уделять много внимания, чтобы не обращаться к невыровненным данным, так как это может быть в несколько раз медленнее. Это ответственность компилятора и разработчика, проектирующего структуры данных. В случае структур данных CLR, компоновка в основном управляется самой средой выполнения. Именно поэтому вы можете заметить много кода в сборщике мусора, связанного с правильной обработкой выравнивания. В Главе 13 вы увидите, как выглядит компоновка памяти объектов и как она может быть контролируема с учетом выравнивания данных.</p>

<p class="justify-style">Однако, начиная с .NET Core 3.0, были введены так называемые аппаратные встроенные функции. Они предоставляют доступ ко многим аппаратно-специфичным инструкциям ЦП, которые не могут быть легко раскрыты с помощью более универсального механизма. Использование выровненной памяти для загрузки и сохранения доступно с тех пор благодаря таким методам, как LoadAlignedVector256 или StoreAligned. Они будут полезны только если вы работаете на очень низком уровне, в основном используя неуправляемую, собственную память через указатели. Они могут быть особенно полезны в так называемых методах векторизации — преобразовании алгоритмов из работы с одним значением на итерацию в работу с набором значений (векторов) на итерацию, с помощью специальных инструкций ЦП SIMD (Single Instruction, Multiple Data).</p>

<p class="h5 p-2">Не временной доступ</p>

<p class="justify-style">До сих пор упоминалось, что в большинстве распространенных типов архитектуры ЦПУ нет доступа к памяти, кроме как через кэш. Вся память, читаемая или записываемая из DRAM процессором, хранится в кэше. Предположим, вы хотите инициализировать очень большой массив, но знаете, что будете использовать его в отдаленном будущем. Из того, что вы узнали до сих пор, вы знаете, что такая инициализация массива вызовет большой трафик памяти. Массив будет записан блоками, одна строка кэша за другой. Более того, каждая из этих операций записи включает три шага – чтение данных в кэш, изменение содержимого кэша и, наконец, запись строки кэша обратно в основную память. В этом сценарии строки кэша заполняются только для записи данных обратно в основную память. Это не только не оптимально само по себе, но и тратит пространство кэша, которое могло бы быть использовано для других программ.</p>

<p class="justify-style">Вы можете избежать такого трафика кэша, используя так называемый набор ассемблерных инструкций не временного доступа – MOVNTI, MOVNTQ, MOVNTDQ и т.д. Они позволяют программисту предотвратить кэширование данных во время записи в память. Они доступны через набор функций C/C++ <code class="fs-6 fst-italic">_mm_stream_*</code>, поэтому для их использования не требуется ассемблер. Например, <code class="fs-6 fst-italic">_mm_stream_si128</code> выполняет инструкцию MOVNTDQ, которая записывает один квадро-слово (4 слова по 4 байта) непосредственно в память. Пример быстрой инициализации массива с использованием этой техники показан в <a href="l-2-2">Листинге 2-2</a>.</p>

<br
<figure id="l-2-2" class="figure">
  <pre class="code border border-secondary">
    <code class="language-cpp">
      #include &lt;emmintrin.h&gt;
      void setbytes(char *p, int c)
      {
        __m128i i = _mm_set_epi8(c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c); // sets 16 
        signed 8-bit integer values
        _mm_stream_si128((__m128i *)&p[0], i);
        _mm_stream_si128((__m128i *)&p[16], i);
        _mm_stream_si128((__m128i *)&p[32], i);
        _mm_stream_si128((__m128i *)&p[48], i);
      }
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-2. Пример использования низкоуровневого API в C++ для не временных записей</figcaption>
</figure>

<p class="justify-style">Упомянутые выше аппаратные встроенные функции доступны с .NET Core 3.0 и также включают возможность использования не временного доступа.</p>

<p class="justify-style">Вы можете использовать набор функций StoreAlignedNonTemporal, которые будут переведены JIT-компилятором в одну из инструкций MOVNTxx, в зависимости от типа данных памяти, к которой вы обращаетесь (байты, целые числа, числа с плавающей запятой и т.д.).</p>

<p class="justify-style">В <a href="#l-2-3">Листинге 2-3</a> вы можете увидеть пример простой программы, которая умножает значения из входного массива на 2 партиями, сохраняя их с помощью вышеупомянутого метода.</p>

<figure id="l-2-3" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      int simdLength = Vector&lt;float&gt;.Count;
      var vec2 = new Vector&lt;float&gt;(2.0f);

      unsafe
      {
        fixed (float* p = outputArray)
        {
          for (; i &lt;= arrayLength - simdLength; i += simdLength)
          {
            var vector = new Vector&lt;float&gt;(inputArray, i);
            vector = vector * vec2;
            vector.StoreAlignedNonTemporal(p + i);
          }
        }
      }
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-3. Пример использования аппаратных встроенных функций для использования не временного доступа</figcaption>
</figure>

<p class="justify-style">Одна сложная вещь, которую нужно помнить, заключается в том, что такие записи должны быть "выровнены". То есть, адреса памяти, к которым вы записываете с помощью StoreAlignedNonTemporal, должны быть кратны размеру строки кэша (32 байта). Это не гарантируется по умолчанию для обычных управляемых массивов float, поэтому вам нужно решить эту проблему двумя возможными способами:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Вместо использования обычного управляемого массива вы можете использовать выровненную нативную память, выделенную с помощью метода NativeMemory.AlignedAlloc (введенного в .NET 6).</p>
    </li>
    <li>
    <p class="justify-style">Вы можете найти первый выровненный адрес в массиве и начать обработку с него. Остальное следует обрабатывать без использования не временного, не выровненного API.</p>
  </li>
</ul>

<p class="justify-style"> В общем, вы должны понимать, что использование не временного доступа - это сложная и хитрая вещь. Это определенно не должно использоваться в качестве стандартной "быстрой техники доступа к памяти".</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Существуют также инструкции загрузки с не временным доступом (nTA) MOVNTDQA, доступные через функции <code class="fs-6 fst-italic">_mm_stream_load_si128</code>. Соответственно, существует набор методов LoadAlignedNonTemporal, доступных через API аппаратных встроенных функций в .NET.</p>
  </div>
</div>

<p class="h5 p-2">Предварительная выборка</p>

<p class="justify-style">Существует еще один механизм, который стремится улучшить использование кэша. Он заключается в заполнении кэша данными, которые, вероятно, понадобятся в ближайшем будущем. Этот механизм называется предварительной выборкой (prefetching), и он может работать в двух разных режимах:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Аппаратно управляемая</span>: Когда ЦПУ замечает несколько промахов кэша с определенными шаблонами. Большинство ЦПУ отслеживают от 8 до 16 шаблонов доступа к памяти (чтобы компенсировать типичную многопоточную/многопроцессный способ работы).</p>
    </li>
    <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Программно управляемая</span>: Через явный вызов инструкции PREFETCHT0, доступной через функцию _mm_prefetch в C/C++.</p>
  </li>
</ul>

<p class="justify-style">Предварительная выборка, как и все другие механизмы кэширования, является обоюдоострым оружием. Если вы хорошо понимаете шаблоны доступа к памяти в вашем коде, то использование предварительной выборки может заметно ускорить производительность вашей программы. С другой стороны, очень сложно быть уверенным, что вы правильно понимаете эти шаблоны доступа к памяти, учитывая очень широкий контекст, в котором работает ваш код – под влиянием других потоков в вашей программе, потоков других программ и потоков самой операционной системы. Время имеет решающее значение: если вы выполните предварительную выборку слишком поздно, данные не будут доступны, когда они вам понадобятся. С другой стороны, если вы выполните предварительную выборку слишком рано, данные могут быть вытеснены из кэша к тому времени, когда вы начнете их использовать. Предварительная выборка используется сборщиком мусора на x86, x64 и ARM64 (см. <a href="#l-2-4">Листинг 2-4</a>).</p>

<figure id="l-2-4" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      // enable on processors known to have a useful prefetch instruction
      #if defined(TARGET_AMD64) || defined(TARGET_X86) || defined(TARGET_ARM64)
      #define PREFETCH
      #endif
      #ifdef PREFETCH
      inline void Prefetch(void* addr)
      {
      #ifdef TARGET_WINDOWS
      #if defined(TARGET_AMD64) || defined(TARGET_X86)
      #ifndef _MM_HINT_T0
      #define _MM_HINT_T0 1
      #endif
        _mm_prefetch((const char*)addr, _MM_HINT_T0);
      #elif defined(TARGET_ARM64)
        __prefetch((const char*)addr);
      #endif //defined(TARGET_AMD64) || defined(TARGET_X86)
      #elif defined(TARGET_UNIX)
        __builtin_prefetch(addr);
      #else //!(TARGET_WINDOWS || TARGET_UNIX)
      UNREFERENCED_PARAMETER(addr);
      #endif //TARGET_WINDOWS
      }
      #else //PREFETCH
      inline void Prefetch (void* addr)
      {
        UNREFERENCED_PARAMETER(addr);
      }
      #endif //PREFETCH
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-4. Части кода .NET, связанные с предварительной выборкой, в зависимости от архитектуры</figcaption>
</figure>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Примером плохого использования кэша может быть ситуация, когда алгоритм сборки мусора спроектирован таким образом, что некоторые очень маленькие, 1-байтовые диагностические данные разбросаны по всей памяти в случайных местах. Операция по сбору этой информации будет очень затратной с точки зрения кэширования. Нам придется заполнять кэш через строки кэша, чтобы прочитать всего один байт.</p>
  </div>
</div>

<p class="justify-style">Алгоритмы, которые интенсивно работают с памятью (а сборка мусора по своей сути работает с памятью), должны учитывать эти внутренние особенности ЦПУ. Память - это не просто плоское пространство, где можно случайным образом выбирать байты здесь и там без каких-либо последствий!</p>

<p class="h5 p-2">Иерархический кэш</p>

<p class="justify-style">Возвращаясь к архитектуре оборудования, из-за требований к производительности, с одной стороны, и оптимизации затрат, с другой, дизайн ЦПУ сегодня эволюционировал в более сложную иерархическую кэш-память. Идея проста. Вместо одного кэша давайте создадим несколько, с разными размерами и скоростями. Это позволяет создать очень маленький и очень быстрый кэш первого уровня (называемый L1), затем немного больший и немного медленнее кэш второго уровня (L2), и, наконец, кэш третьего уровня (L3). В современной архитектуре эта нумерация заканчивается на трех уровнях. Такая иерархическая кэш-память современных компьютеров показана на <a href="#f-2-10">Рисунке 2-10</a>. Правда, иногда можно встретить процессоры, оснащенные кэшем L4, но это немного другой вид памяти, предназначенный в основном для интегрированных графических карт внутри этих ЦПУ.</p>

<figure id="f-2-10" class="figure">
  <img src="content/img/2-10.png" class="img-fluid" alt="Рисунок 2-10" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-10. ЦПУ с иерархическим кэшем – кэш первого уровня разделен на кэш инструкций (L1i) и кэш данных (L1d), а также кэш второго (L2) и третьего (L3) уровней. ЦПУ подключен к ОЗУ через шину памяти</figcaption>
</figure>

<p class="justify-style">Кэш первого уровня разделен на два отдельных блока. Один предназначен для данных (обозначен как L1d), а другой для инструкций (обозначен как L1i). Инструкции, считываемые из памяти и выполняемые процессором, также фактически являются данными, но интерпретируются соответствующим образом. Данные и инструкции кода на уровнях выше L1 фактически обрабатываются одинаково. Однако практика показала, что предпочтительнее обрабатывать данные и инструкции отдельно для самого нижнего уровня кэша. Это подход архитектуры Гарварда. По этой причине архитектура современных компьютеров называется <span class="fw-bold fst-italic">модифицированной Гарвардской архитектурой</span>. Это решение хорошо работает из-за сильной независимости использования областей памяти для хранения данных и программного кода, но только на самом нижнем уровне.</p>

<p class="justify-style">Зная, что существует три основных уровня кэша, возникает очевидный вопрос: каковы типичные различия в скорости и размере между ними и основной памятью? Память на нижних уровнях кэша может быть очень быстрой, настолько, что доступ к L1 и даже L2 может быть быстрее времени выполнения конвейера (если только вам не нужно ждать точного вычисления адреса, что также является дорогостоящей операцией). Итак, каковы эти временные интервалы?</p>

<p class="justify-style">На момент написания этой главы использовался ноутбук с процессором Intel Core i7-4712MQ (поколение Haswell), работающим на частоте 2,30 ГГц. Предполагая, что один цикл процессора на моем ноутбуке занимает примерно 0,4 нс (~1/2,30 ГГц) и используя спецификацию Haswell i7, задержка доступа к различным уровням памяти может быть представлена, как показано в <a href="#t-2-2">Таблице 2-2</a>.</p>

<figure id="t-2-2" class="figure">
  <table class="table table-striped table-hover border-1 ">
    <thead>
      <tr>
        <th scope="col">#</th>
        <th scope="col">Операция</th>
        <th Scope="col">Задержка</th>

            </tr>
          </thead>
          <tbody>
            <tr>
        <th scope="row">1</th>
        <td>Кэш L1</td>
        <td> &lt; 2.0 нс</td>

            </tr>
            <tr>
        <th scope="row">2</th>
        <td>Кэш L2</td>
        <td>4.8 нс</td>
            </tr>
            <tr>
        <th scope="row">3</th>
        <td>Кэш L3</td>
        <td>14.4 нс</td>
            </tr>
            <tr>
        <th scope="row">4</th>
        <td>Основная память</td>
        <td>71.4 нс</td>
            </tr>
            <tr>
        <th scope="row">5</th>
        <td>HDD</td>
        <td>150 000 нс</td>
            </tr>
          </tbody>
        </table>
        <figcaption class="figure-caption">Таблица 2-2. Задержка доступа к различным частям памяти</figcaption>
</figure>

<p class="justify-style">Вы можете ясно видеть, что стоит бороться за оптимальное использование кэша. Задержка может быть в пять раз быстрее, когда необходимые данные доступны в кэше L3, а не в оперативной памяти. С кэшем L1 это более чем в 30 раз лучше. Вот почему чрезвычайно важно для общей производительности знать, как используется кэш. Сколько данных помещается в кэш? Все зависит от конкретной модели процессора, но спецификация i7-4770K довольно хорошо отражает рыночные стандарты. Кэш L1 имеет 64 КБ данных (разделенных на 32 КБ для кода и 32 КБ для данных), в то время как кэш L2 имеет 256 КБ. Кэш L3, всегда намного больше, составляет 8 МБ.</p>

<p class="justify-style">Каково влияние этих временных интервалов в управляемом мире .NET? Давайте рассмотрим простой пример, показывающий задержку при доступе к данным в зависимости от объема обрабатываемой памяти. Мы используем код из <a href="#l-2-5">Листинга 2-5</a>, который выполняет серию последовательных чтений (оптимальный случай). Поскольку используемая структура имеет размер 64 байта, чтение выполняется с шагом 64 байта, и каждый раз необходимо загружать новую строку кэша. На <a href="#f-2-11">Рисунке 2-11</a> показаны средние времена доступа к одному элементу массива tab в зависимости от того, сколько памяти этот массив занимал в целом.</p>

<p class="justify-style">Очевидно ухудшение времени доступа, когда размер данных превышает размер кэша каждого уровня. Поскольку тесты проводились на процессоре Intel i7-4770K, четко видны точки деградации производительности около 256 КБ и 8192 КБ, что соответствует размерам кэша L2 и L3. Вы можете видеть, что работа с небольшими размерами данных может быть в несколько раз быстрее, чем работа с данными, которые не помещаются в кэш L3.</p>

<br>
<figure id="l-2-5" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      public struct OneLineStruct
      {
        public long data1;
        public long data2;
        public long data3;
        public long data4;
        public long data5;
        public long data6;
        public long data7;
        public long data8;
      }

      public static long OneLineStructSequentialReadPattern(OneLineStruct[] tab)
      {
        long sum = 0;
        int n = tab.Length;
        for (int i = 0; i &lt; n; ++i)
        {
          unchecked { sum += tab[i].data1; }
        }
        return sum;
      }
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-5. Последовательное чтение следующих строк кэша</figcaption>
</figure>

<figure id="f-2-11" class="figure">
  <img src="content/img/2-11.png" class="img-fluid" alt="Рисунок 2-11" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-11. Время доступа в зависимости от размера данных – архитектура Intel x86/последовательное чтение. Обратите внимание: обе оси логарифмические</figcaption>
</figure>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Существует одна интересная, но не столь важная тема в контексте кэша – стратегии вытеснения. Речь идет о том, как освободить место для новых данных, если их нет на данном уровне. Существуют два возможных подхода, иногда смешиваемых на разных уровнях:</p>
    <ul class="bullet-list ms-1">
      <li>
        <p class="justify-style"><span class="fw-bold fst-italic">Эксклюзивный кэш</span>: Данные находятся только на одном уровне кэша. Этот метод чаще всего используется в процессорах AMD.</p>
      </li>
      <li>
        <p class="justify-style"><span class="fw-bold fst-italic">Инклюзивный кэш</span>: Когда каждая строка кэша на нижнем уровне (например, L1d) также присутствует на более высоком уровне (например, L2).</p>
      </li>
    </ul>
    <p class="card-text justify-style">Хотя это интересно, но это не влияет на ваше понимание управления памятью. Следует предположить, что производители процессоров делают все возможное, чтобы обеспечить наиболее эффективную реализацию этих механизмов.</p>
  </div>
</div>

<p class="h5 p-2">Многоядерный иерархический кэш</p>

<p class="justify-style">Однако это не конец вашего путешествия по дизайну компьютеров. Большинство современных процессоров имеют более одного ядра. В упрощенных терминах, ядро – это то, что представляет собой отдельный, упрощенный процессор – оно может выполнять код независимо от других ядер. В прошлом каждое ядро выполняло ровно один поток одновременно. Таким образом, четырехъядерный процессор мог выполнять четыре потока одновременно. В настоящее время практически все процессоры имеют механизм одновременной многопоточности (SMT), позволяющий одновременно выполнять два потока в одном ядре. Для процессоров Intel это называется гиперпоточностью, а полная поддержка SMT была добавлена в микроархитектуру AMD Zen. Распределение кэшей между отдельными ядрами в примере четырехъядерного процессора показано на <a href="#f-2-12">Рисунке 2-12</a>.</p>

<figure id="f-2-12" class="figure">
  <img src="content/img/2-12.png" class="img-fluid" alt="Рисунок 2-12" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-12. Многоядерный процессор – каждое ядро имеет свой кэш первого уровня, разделенный на кэш инструкций (L1i) и кэш данных (L1d), и кэш второго уровня (L2). Кэш третьего уровня (L3) общий для всех ядер. Процессор подключен к ОЗУ через шину памяти</figcaption>
</figure>

<p class="justify-style">Как видите, каждое ядро имеет свой собственный кэш первого и второго уровня. Кэш третьего уровня общий для всех ядер. Как ядра и кэш L3 взаимосвязаны, на самом деле является деталью реализации. Например, в большинстве современных процессоров Intel существует двунаправленная, чрезвычайно быстрая шина шириной 32 байта, которая дополнительно соединяет их с интегрированным графическим процессором и системным агентом. Обратите внимание, что для процессоров с SMT два потока, работающие на одном ядре, делят кэш L1 и L2, поэтому их фактическое использование делится пополам, если только оба потока не нуждаются в доступе к одним и тем же данным. Это, очевидно, требует поддержки операционной системы для целенаправленного назначения потоков ядрам на основе их шаблонов доступа к памяти.</p>

<p class="justify-style">Поскольку каждый поток может выполняться на отдельном процессоре и/или ядре, возникает проблема согласованности кэшированных данных. Каждое ядро имеет свою собственную версию кэшей первого и второго уровней, и только третий уровень является общим. Это требует введения сложной концепции, известной как согласованность кэша. Этот механизм описывает, как поддерживается согласованность хранимых данных, и применяется с помощью протокола согласованности кэша – способа информирования о изменениях данных между ядрами. Всякий раз, когда данные в локальном кэше были изменены (поддерживаются с помощью флага загрязнения или модификации), эта информация должна быть передана другим ядрам.</p>

<p class="justify-style">Существует множество расширений и продвинутых протоколов согласованности кэша, которые предназначены для обеспечения эффективной работы – в частности, очень популярный протокол MESI. Его название происходит от названий четырех состояний, в которых может находиться строка кэша – модифицированное, эксклюзивное, общее и недействительное. Тем не менее, протоколы согласованности кэша могут накладывать большую нагрузку на трафик памяти и, следовательно, на общую производительность программы. Интуитивно понятно, что постоянная необходимость взаимного обновления кэша между ядрами может привести к заметной нагрузке. Код, который вы пишете, должен стараться минимизировать любой доступ с разных ядер к адресам памяти в пределах одних и тех же строк кэша. Это означает попытку избежать коммуникации между потоками или, по крайней мере, уделять много внимания тому, какие данные и как эти данные разделяются между потоками.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Поскольку упомянутые ранее не временные инструкции пропускают обычные правила согласованности кэша, их использование должно сопровождаться специальной ассемблерной инструкцией <code class="fs-6 fst-italic">sfence</code>, чтобы сделать их результаты видимыми для других ядер.</p>
  </div>
</div>

<p class="justify-style">Но опять же, полезны ли эти знания в высокоуровневых средах, таких как .NET? Может ли сборщик мусора, обладающий знаниями о внутренних механизмах, скрыть эти детали реализации оборудования? Ответы на эти вопросы можно найти в следующем примере.</p>

<p class="justify-style"><a href="#l-2-6">Листинг 2-6</a> показывает многопоточный код, который может одновременно запускать количество потоков, равное threadsCount, обращающихся к одному и тому же массиву sharedData. Каждый поток просто увеличивает один элемент массива, теоретически не влияя на другие потоки. В этом примере есть два важных параметра, указывающих, как эти элементы расположены в общем массиве – есть ли начальный зазор и насколько они удалены друг от друга (смещение). Поскольку этот код выполняется при threadsCount=4 на четырех-ядерной машине, вероятно, что каждый поток будет выполняться на своем физическом ядре.</p>

<figure id="l-2-6" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      const int offset = 1;
      const int gap = 0;
      public static int[] sharedData = new int[4 * offset + gap * offset];
      public static long DoFalseSharingTest(int threadsCount, int size = 100_000_000)
      {
        Thread[] workers = new Thread[threadsCount];
        for (int i = 0; i &lt; threadsCount; ++i)
        {
          workers[i] = new Thread(new ParameterizedThreadStart(idx =>
          {
            int index = (int)idx + gap;
            for (int j = 0; j &lt; size; ++j)
            {
              sharedData[index * offset] = sharedData[index * offset] + 1;
            }
          }));
        }
        for (int i = 0; i &lt; threadsCount; ++i)
          workers[i].Start(i);
        for (int i = 0; i &lt; threadsCount; ++i)
          workers[i].Join();
        return 0;
      }
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-6. Возможность ложного совместного использования между потоками</figcaption>
</figure>

<figure id="t-2-3" class="figure">
  <table class="table table-striped table-hover border-1 ">
    <thead>
      <tr>
        <th scope="col">#</th>
        <th scope="col">Version</th>
        <th Scope="col">ПК</th>
        <th Scope="col">Ноутбук</th>
        <th Scope="col">Raspberry Pi 2</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th scope="row">1</th>
        <td>(offset=1, gap=0)</td>
        <td>5.0s</td>
        <td>6.7s</td>
        <td>29.0s</td>
      </tr>
      <tr>
        <th scope="row">2</th>
        <td>(offset=16, gap=0)</td>
        <td>2.4s</td>
        <td>2.6s</td>
        <td>13.8s</td>
      </tr>
      <tr>
        <th scope="row">3</th>
        <td>(offset=16, gap=16)</td>
        <td>0.7s</td>
        <td>0.8s</td>
        <td>12.1s</td>
      </tr>
    </tbody>
  </table>
  <figcaption class="figure-caption">Таблица 2-3. Результаты тестов кода из <a href="#l-2-6">Листинга 2-6</a>, показывающие влияние ложного совместного использования на время обработки</figcaption>
</figure>

<p class="justify-style">В <a href="#t-2-3">Таблице 2-3</a> вы можете увидеть значительные различия в производительности между различными комбинациями зазора и смещения. В наиболее распространенном случае зазор равен 0, а смещение равно 1. Макет и доступы потоков показаны на <a href="#f-2-13a">Рисунке 2-13a</a>. К сожалению, это вводит очень большую нагрузку на согласованность кэша. Каждый поток (ядро) имеет свою локальную копию одного и того же региона памяти (в своей строке кэша), поэтому после каждого увеличения он должен аннулировать локальные копии других. Это заставляет ядра постоянно аннулировать свои кэши.</p>

<figure id="f-2-13a" class="figure">
  <img src="content/img/2-13a.png" class="img-fluid" alt="Figure 2-13a" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-13a. Версия #1 с смещением 1 байт и без зазора – каждый поток изменяет одну и ту же строку кэша</figcaption>
</figure>

<p class="justify-style">Очевидное решение этой проблемы заключается в том, чтобы распределить элементы, к которым обращается каждый поток, по разным строкам кэша. Самый простой способ - создать гораздо больший массив и использовать только каждый 16-й элемент (16 раз по 4 байта одного Int32 составляют 64 байта). Это показано в примере со смещением 16 и зазором 0 (см. <a href="#f-2-13b">Рисунок 2-13b</a>). Как видно из <a href="#t-2-3">Таблицы 2-3</a>, производительность значительно лучше, но все еще может быть улучшена.</p>

<figure id="f-2-13b" class="figure">
  <img src="content/img/2-13b.png" class="img-fluid" alt="Figure 2-13b" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-13b. Версия #2 с смещением 16 байт и без зазора – каждый поток обращается и изменяет свою собственную строку кэша</figcaption>
</figure>

<p class="justify-style">На первый взгляд это не очевидно, но все еще существует одна строка кэша, которая постоянно аннулируется, что приводит к проблеме, известной как ложное совместное использование – неудачный шаблон доступа к данным, при котором общие данные, которые теоретически не изменяются, находятся в пределах строки кэша, изменяемой другим потоком, вызывая ее постоянное аннулирование. Как вы узнаете в следующей главе, каждый тип в .NET имеет некоторый дополнительный заголовок, прикрепленный к его началу. В случае массивов длина хранится в начале объекта. Более того, при доступе к элементам массива с помощью оператора индекса сгенерированный код проверяет, не выходит ли он за пределы массива. Это означает чтение начала объекта массива для проверки длины массива каждый раз при доступе к любому элементу массива. Поэтому первое ядро делит начало объекта с другими ядрами, постоянно аннулируя соответствующие строки кэша. Чтобы исправить это, необходимо сместить элементы на одну строку кэша. Это версия, когда смещение все еще равно 16, но зазор также равен 16 (см. <a href="#f-2-12c">Рисунок 2-13c</a>).</p>

<figure id="f-2-13c" class="figure">
  <img src="content/img/2-13c.png" class="img-fluid" alt="Figure 2-13c" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-13c. Версия #3 с смещением 16 байт и зазором 16 байт – каждый поток изменяет свою собственную строку кэша и читает общую строку кэша с заголовком массива</figcaption>
</figure>
  
<p class="justify-style"> В этом случае каждое ядро имеет свою собственную локальную копию первой строки кэша только для чтения. И оно изменяет свои собственные строки кэша с данными. Никакой дополнительной нагрузки на протокол согласованности кэша не добавляется. Из <a href="#t-2-3">Таблицы 2-3</a> видно, что такой код работает даже в семь раз быстрее, чем с ложным совместным использованием!</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Другие архитектуры иногда отказываются от последовательной согласованности, присутствующей в x86, что упрощает их дизайн, но усложняет программирование (требуются явные барьеры памяти). Примером такой архитектуры является PowerPC 2006 года на компьютерах Apple.</p>
  </div>
</div>

<p class="justify-style">До сих пор вы потратили много времени на понимание кэширования данных. Однако несколько страниц назад упоминалось, что существует также кэш для программных инструкций (L1i). Мы не рассматривали его по нескольким причинам. Во-первых, компиляторы хорошо справляются с правильной подготовкой кода, а процессоры также хорошо угадывают шаблоны доступа к коду. В результате этот кэш работает хорошо – компилятор и природа выполнения программы обеспечивают хорошую временную и пространственную локальность, которую может использовать процессор. Более того, управление кэшем инструкций не относится к области управления памятью в .NET, которая сосредоточена на управлении данными. Единственное очевидное улучшение – это генерировать как можно меньший код. Однако сегодня трудно применить этот совет на практике – все делается за счет оптимизаций компилятора, а размер кода скорее определяется бизнес-потребностями.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Тем не менее, даже в .NET мы можем проектировать вызовы методов с учетом промахов кэша L1i. Это в основном включает избегание большого количества виртуальных вызовов и предпочтение повторяющихся вызовов одного и того же метода для большого набора данных. Мы увидим такой пример в Главе 10.</p>
  </div>
</div>

<hr class="border-2">

<p id="chapter2-2" class="h3 p-2">Операционная система</p>

<p class="justify-style">Вы уже довольно много времени провели, изучая аппаратное обеспечение. Изначально мы обещали рассмотреть операционную систему. Сейчас пришло время обсудить, как разработчики операционной системы серьезно учли все эти аппаратные ограничения.</p>

<p class="justify-style">Из-за архитектуры операционной системы и аппаратного обеспечения, физические ограничения памяти варьируются от 2 ГБ до 24 ТБ. И типичное потребительское оборудование в настоящее время оснащено десятками гигабайт памяти. Если бы данная программа должна была использовать физическую память напрямую, ей пришлось бы управлять всеми создаваемыми и удаляемыми областями памяти. Такая логика управления памятью была бы не только сложной, но и повторялась бы в каждой программе. Более того, с точки зрения низкоуровневого программирования, использование памяти таким образом было бы неудобным. Каждая программа должна была бы помнить, какие области памяти она использует, чтобы программы не мешали друг другу. Аллокаторы должны были бы сотрудничать для правильного управления созданными и удаленными объектами. Это также довольно опасно с точки зрения безопасности – без какого-либо промежуточного слоя программа могла бы получить доступ не только к своим собственным областям памяти, но и ко всем данным других процессов.</p>

<hr class="border-2">

<p id="chapter2-2-1" class="h4 p-2">Виртуальная память</p>

<p class="justify-style">Таким образом, была введена очень удобная абстракция – виртуальная память. Она перемещает логику управления памятью в операционную систему, которая предоставляет так называемое виртуальное адресное пространство любой программе. В частности, это означает, что каждый процесс думает, что он единственный, работающий в системе, и что вся память доступна для его собственных нужд. Еще лучше, поскольку адресное пространство виртуально, оно может быть больше, чем физическая память. Кроме того, физическая оперативная память DRAM может быть расширена с помощью вторичного хранилища, такого как жесткие диски, используемые для файла подкачки – об этом позже.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Существуют ли операционные системы без виртуальной памяти? Для любого массового использования – нет. Но да, существуют очень маленькие операционные системы и фреймворки, предназначенные для встроенных систем, такие как ядро μClinux.</p>
  </div>
</div>

<p class="justify-style">Менеджер памяти операционной системы выполняет две основные задачи:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Отображение виртуального адресного пространства в физическую память</span>: Виртуальные адреса имеют длину 32 бита на 32-битных машинах и 64 бита на 64-битных машинах (хотя в настоящее время используется только нижние 48 бит, что все еще позволяет адресное пространство размером 128 ТБ).</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Перемещение некоторых областей памяти из оперативной памяти на жесткие диски</span>: Поскольку общий объем используемой памяти может быть больше физической памяти, должен существовать механизм для перемещения данных из оперативной памяти для временного хранения на более медленных носителях, таких как HDD или SSD. Эти данные хранятся в файле подкачки или swap-файле.</p>
  </li>
</ul>

<p class="justify-style">Перемещение данных из оперативной памяти для временного хранения на более медленные носители, очевидно связано с значительным снижением производительности. Этот процесс в разных системах называется свопингом или пейджингом, в основном по историческим причинам. В Windows есть специальный файл, называемый файлом подкачки, который хранит данные, поступающие из памяти, отсюда и термин пейджинг. В Linux такие данные хранятся на выделенном разделе, называемом разделом подкачки, отсюда и термин свопинг в Unix-подобных системах.</p>

<p class="justify-style">Виртуальная память реализуется в ЦПУ (с помощью блока управления памятью – MMU) при сотрудничестве с ОС. Было бы слишком дорого отображать виртуальное пространство в физическое побайтно, поэтому это делается непрерывными блоками памяти, называемыми страницами. Схематическая иллюстрация виртуальной памяти и физической памяти показана на <a href="#f-2-14">Рисунке 2-14</a>.</p>

<figure id="f-2-14" class="figure">
  <img src="content/img/2-14.png" class="img-fluid" alt="Figure 2-14" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-14. Отображение виртуальных страниц в физические. Каждый процесс (A - светло-серый и B - темно-серый) видит свое собственное виртуальное адресное пространство, но физически их страницы хранятся как в ОЗУ (сплошные страницы), так и на диске (штрихованные страницы)</figcaption>
</figure>

<p class="justify-style">Существует также понятие каталога страниц, поддерживаемого ОС для каждого процесса, который позволяет отображать виртуальный адрес в физический. Проще говоря, записи каталога страниц указывают на физические начальные адреса страниц и другие метаданные, такие как привилегии.</p>

<p class="justify-style">В современных операционных системах часто используется подход с многоуровневыми каталогами. 
  Это позволяет компактно хранить разреженные данные каталога страниц, сохраняя при этом небольшой размер страницы. В настоящее время 
  на большинстве архитектур типичный размер страницы составляет 4 КБ (включая x86, x64 и ARM) и используется четырехуровневый каталог страниц 
  (см. <a href="#f-2-15">Рисунок 2-15</a>).</p>

<figure id="f-2-15" class="figure">
  <img src="content/img/2-15.png" class="img-fluid" alt="Figure 2-15" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-15. Четырехуровневый каталог страниц с размером страницы 4 кБ – три уровня селектора страниц позволяют представлять гораздо более разреженные данные</figcaption>
</figure>

<p class="justify-style">Когда виртуальный адрес переводится в физический адрес, требуется обход каталога страниц:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Селектор уровня 1 выбирает запись в каталоге уровня 1, которая указывает на одну из записей каталога уровня 2.</p>
    </li>
    <li>
    <p class="justify-style">Селектор уровня 2 выбирает запись в конкретной записи каталога уровня 2, которая указывает на одну из записей каталога уровня 3.</p>
    </li>
    <li>
    <p class="justify-style">Селектор уровня 3 выбирает запись в конкретной записи каталога уровня 3, которая указывает на одну из записей каталога уровня 4.</p>
    </li>
    <li>
    <p class="justify-style">В конечном итоге селектор уровня 4 выбирает запись в конкретной записи каталога уровня 4, которая указывает непосредственно на страницу в физической памяти.</p>
    </li>
    <li>
    <p class="justify-style">Смещение указывает на конкретный адрес в выбранной странице.</p>
  </li>
</ul>

<p class="justify-style">Такой перевод требует обхода дерева каталога страниц, который хранится в физической памяти. Это означает, что он также может быть кэширован в кэшах L1/L2/L3. Но все равно это приведет к огромным накладным расходам, если каждое преобразование адреса (операция, выполняемая очень часто) потребует доступа к этим данным. Таким образом, были введены буферы трансляции Look-Aside (TLB), которые кэшируют сам перевод. Идея проста — TLB работает как карта, где селектор является ключом, а начало физического адреса страницы — значением. TLB созданы так, чтобы быть чрезвычайно быстрыми, поэтому они малы с точки зрения хранения. Они также являются многоуровневыми, чтобы имитировать структуру каталога страниц. Результатом промаха TLB (не кэшированного виртуального в физическое преобразование) является выполнение полного обхода каталога страницы, что является дорогостоящим.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Как и в случае со всеми кэшами, предварительная выборка TLB сложна — если сам процессор должен быть тем, кто запускает предварительную выборку (например, из-за предсказания ветвления), он может вызвать ненужное обход каталога страниц (поскольку предсказание ветвления может быть неверным). Чтобы предотвратить это, предварительное получение TLB контролируется программным обеспечением. Кроме того, имейте в виду, что предварительная загрузка ограничена оборудованием по количеству страниц. В противном случае это приведет к промаху страницы, что будет большим ненужным накладным расходом, если догадка окажется неверной. Существуют ли какие-либо релевантные оптимизации TLB, о которых следует помнить в вашем коде? Цель в основном состоит в том, чтобы уменьшить количество страниц, чтобы каталог страниц оставался маленьким, что, в свою очередь, уменьшило бы количество промахов TLB. С точки зрения .NET приложения, большие страницы, описанные в следующем разделе, являются единственным способом влияния на управление страницами.</p>
    <p class="card-text justify-style">Как правило, L1 работает с виртуальными адресами, потому что стоимость их трансляции в физические адреса будет намного выше, чем сам доступ к кэшу. Это означает, что при изменении страницы все или некоторые строки кэша должны быть аннулированы. Поэтому большое количество изменений страниц негативно влияет на производительность кэша.</p>
  </div>
</div>

<hr class="border-2">
<p id="chapter2-2-2" class="h4 p-2">Большие страницы</p>

<p class="justify-style">Как было показано ранее, перевод виртуальных адресов может быть дорогостоящим, и было бы здорово избегать его как можно чаще. Одним из подходов может быть использование большего размера страницы. Это потребовало бы меньшего количества переводов адресов, потому что многие адреса поместились бы на одной странице, а перевод уже был бы кэширован в TLB. Но большие страницы могут привести к пустой трате оперативной памяти или дискового пространства, используемого файлом подкачки. Решение одно – так называемые большие (или огромные) страницы. Благодаря аппаратной поддержке они позволяют создавать большой, непрерывный блок физической памяти, состоящий из множества последовательно уложенных обычных страниц. Эти страницы обычно на два-три порядка больше, чем обычная страница. Они могут быть полезны в сценариях, когда программе требуется произвольный доступ к гигабайтам данных. Ядра баз данных являются примерами потребителей больших страниц. Операционная система Windows также сопоставляет образы и данные ядра с большими страницами. Большая страница не является выгружаемой (не может быть перемещена в файл подкачки) и поддерживается как в Windows, так и в Linux. К сожалению, заполнить большую страницу довольно сложно: это приводит к фрагментации. Кроме того, может отсутствовать доступный непрерывный диапазон физической памяти.</p>

<p class="justify-style">Начиная с .NET Core 3.0, большие страницы можно включить в .NET с помощью параметра COMPlus_GCLargePages/DOTNET_ GCLargePages. Более подробно об этом будет рассказано в главе 11.</p>

<hr class="border-2">
<p id="chapter2-2-3" class="h4 p-2">Фрагментация виртуальной памяти</p>

<p class="justify-style">Как всегда, когда дело доходит до выделения и освобождения памяти, фрагментация может проявиться, как было упомянуто в главе 1 при обсуждении концепции кучи. В случае виртуальной памяти это означает, что операционная система не сможет зарезервировать непрерывный блок памяти заданного размера в адресном пространстве процесса. Между уже зарезервированной памятью нет достаточно большого промежутка, хотя суммарный размер всех свободных промежутков может значительно превышать требуемый размер.</p>

<p class="justify-style">Эта проблема может быть серьезной для 32-разрядных приложений, где виртуальное пространство может быть слишком маленьким для современных потребностей. Фрагментация может быть особенно острой, когда процесс выделяет большие сегменты памяти и работает довольно долго: именно с такой ситуацией приходится иметь дело в веб-приложениях .NET в 32-разрядной версии (размещенной в IIS). Чтобы предотвратить фрагментацию, процесс должен правильно управлять памятью (а для процесса .NET это означает саму среду CLR). Мы углубимся в эти детали при описании алгоритмов сборки мусора в главах 7–10, так как это требует более глубокого понимания самой .NET.</p>

<hr class="border-2">

<p id="chapter2-2-4" class="h4 p-2">Общая компоновка памяти</p>

<p class="justify-style">Зная базовый блок сборщика памяти, теперь можно перейти к обсуждению памяти на более высоком уровне. Что такое структура памяти программы? При описании типичной структуры памяти программы часто используется рисунок, как показано на <a href="#f-2-16">рисунке 2-16</a>. Он показывает типичное адресное пространство программы.</p>

<figure id="f-2-16" class="figure">
  <img src="content/img/2-16.png" class="img-fluid" alt="Figure 2-16" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-16. Виртуальное адресное пространство типичного процесса</figcaption>
</figure>

<p class="justify-style">Как видно, виртуальное адресное пространство разделено на две области:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Пространство ядра</span>: Верхний диапазон адресов занимает сама операционная система. Оно известно как пространство ядра, потому что именно здесь сопоставляются драйвер устройства и структуры данных ядра. Обратите внимание, что только код ядра может получить доступ к этой части адресного пространства.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Пространство пользователя</span>: Для кода приложения доступна нижняя часть адресного пространства (за исключением самой нижней, используемой для определения нулевых указателей).</p>
  </li>
</ul>

<p class="justify-style">С вашей точки зрения, конечно, интересно только пространство пользователя: именно здесь CLR хранит ваши данные. Благодаря механизму виртуальной памяти каждый процесс имеет доступ ко всему адресному пространству – как если бы он был единственным процессом в системе.</p>

<p class="justify-style">Вот области памяти, видимые на <a href="#f-2-17">рисунке 2-17</a>:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Для каждого потока создается стек.</p>
  </li>
  <li>
    <p class="justify-style">Операционная система предоставляет различные API для среды выполнения для создания куч (подробнее об этом позже).</p>
  </li>
  <li>
    <p class="justify-style">При запуске приложения операционная система сопоставляет соответствующий двоичный файл с диска в адресное пространство. Некоторые данные, доступные только для чтения, могут совместно использоваться процессами, например, исполняемый ассемблерный код.</p>
  </li>
  <li>
    <p class="justify-style">Такой рисунок полезен для высокоуровневого представления общей схемы адресного пространства. Но реальность сложнее. Давайте углубимся в контекст двух основных операционных систем, поддерживаемых средой выполнения .NET: Windows и Linux.</p>
  </li>
</ul>

<hr class="border-2">

<p id="chapter2-2-5" class="h4 p-2">Управление памятью в Windows</p>

<p class="justify-style">Виртуальное адресное пространство зависит от версии системы. Краткое изложение этих ограничений приведено в <a href="t-2-4">таблице 2-4</a>.</p>

<figure id="t-2-4" class="table">
  <table class="table table-bordered">
    <thead>
      <tr>
        <th scope="col">Тип процесса</th>
        <th scope="col">Windows (32-Bit)</th>
        <th scope="col">Windows 8/Server 2012</th>
        <th scope="col">Windows 8.1+/Server 2012+</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>32-bit</td>
        <td>2/2 GB</td>
        <td>2/2 GB</td>
        <td>2/2 GB</td>
      </tr>
      <tr>
        <td> 32-bit (*)</td>
        <td>3/1 GB</td>
        <td>4 GB/8 TB</td>
        <td>4 GB/128 TB</td>
      </tr>
      <tr>
        <td>64-bit</td>
        <td>-</td>
        <td>8/8 TB</td>
        <td>128/128 TB</td>
      </tr>
    </tbody>
  </table>
  <figcaption class="figure-caption"><p>Таблица 2-4. Ограничения на размер виртуального адресного пространства в Windows (пользователь/ядро)</p>
    <p> * Большой флаг учета адресов (также известный как переключатель /3GB)</p>
  </figcaption>
</figure>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Существует механизм, называемый Address Windowing Extensions (AWE), который позволяет получить доступ к большему количеству физической памяти, чем указано здесь, а затем отобразить только ее части в виртуальное адресное пространство через «окно AWE». Это может быть особенно полезно в 32-разрядной среде для преодоления ограничения в 2 или 3 ГБ на процесс. Однако в данном случае это не имеет значения, поскольку среда CLR не использует эту функцию.</p>
  </div>
</div>

<p class="justify-style">Ограничения размера виртуальной памяти одного процесса стали болезненными в конце господства 32-разрядных систем. Ограничение до 2 ГБ (или до 3 ГБ в расширенном режиме) может быть проблематичным в крупных корпоративных приложениях. Классическим примером является веб-приложение ASP.NET, размещенное в IIS на 32-разрядных компьютерах Windows Server. Если этот предел должен был быть достигнут, не оставалось другого выбора, кроме как перезапустить все веб-приложение. Это вынудило горизонтальное масштабирование в больших веб-системах, создав несколько экземпляров серверов, которые обрабатывают меньше трафика и, следовательно, потребляют меньше памяти. В настоящее время в мире доминируют 64-битные системы, и ограниченное виртуальное адресное пространство больше не является проблемой. Но однако, обратите внимание, что 32-разрядная скомпилированная программа имеет ограничение виртуального адресного пространства в 4 ГБ даже на 64-разрядных серверах Windows.</p>

<p class="justify-style">Менеджер памяти Windows предоставляет два основных API:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Виртуальная память</span>: Это низкоуровневый API, который работает с самим адресным пространством для резервирования и фиксации страниц. Функции VirtualAlloc и VirtualFree — это функции, используемые средой CLR.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Куча</span>: Высокоуровневый API, предоставляющий Allocator (вспомните его из главы 1), используемый средой выполнения C и C++ для реализации функций malloc/free и new/delete. Этот слой включает, среди прочего, функции HeapAlloc и HeapFree.</p>
  </li>
</ul>

<p class="justify-style">Поскольку CLR имеет собственную реализацию Allocator для создания объектов .NET (которую вы подробно увидите в главе 6), используются только виртуальные API. В двух словах, среда CLR запрашивает у операционной системы дополнительные страницы, и соответствующее размещение объектов на этих страницах обрабатывается самостоятельно. CLR создает свои собственные внутренние структуры данных в машинном коде с помощью оператора C++ new (который сам построен на основе API Heap).</p>

<p class="justify-style">В Windows API виртуальной памяти работает в два этапа. Во-первых, вы резервируете непрерывный диапазон памяти в адресном пространстве, вызывая VirtualAlloc с MEM_RESERVE и PAGE_READWRITE (поскольку вы хотите иметь возможность хранить и считывать данные оттуда) в качестве параметров. Диспетчер памяти Windows возвращает адрес, с которого начинается зарезервированный диапазон памяти в адресном пространстве. Будьте внимательны: вы пока не можете получить доступ к этой памяти! Во-вторых, когда вам нужно прочитать или записать в эту память, вы должны зафиксировать нужные страницы из этого зарезервированного диапазона. Вам не нужно фиксировать весь диапазон, только нужные страницы. Именно этим и занимается среда CLR: резервирует большие диапазоны в адресном пространстве один раз, а затем фиксирует страницы, когда требуется место для выделения дополнительных объектов. При первом обращении к зафиксированной странице диспетчер памяти гарантирует, что она инициализирована нулевыми строками.</p>

<p class="justify-style">Когда объекты собраны и обнаружено достаточное количество свободного места, соответствующие страницы удаляются путем вызова VirtualFree с MEM_DECOMMIT в качестве параметра, чтобы вернуть память менеджеру памяти Windows. Точные детали объясняются в главе 10.</p>

<p class="justify-style">Следующий вопрос, который вы, возможно, захотите задать, — как измерить потребление «памяти» вашими приложениями. Исходя из того, что уже было объяснено, память, в которой хранятся ваши объекты, называется частной выделенной памятью вашего процесса. Общая выделенная память подсчитывает данные только для чтения, такие как ассемблерный код, хранящийся в исполняемых файлах, и не представляет интереса в данном контексте.</p>

<p class="justify-style">Более того, приватные страницы также могут быть заблокированы, что заставляет их оставаться в физической памяти (не будут перемещены в файл подкачки) до явной разблокировки или завершения работы приложения. Блокировка может быть полезна для критически важного для производительности пути в программе. Мы рассмотрим пример использования блокировки страниц в пользовательском узле CLR, показанный в главе 15.</p>

<p class="justify-style">Резервированные и выделенные страницы управляются процессом с помощью вышеупомянутых вызовов методов ><span class="fw-bold fst-italic">VirtualAlloc/VirtualFree</span> и ><span class="fw-bold fst-italic">VirtualLock/VirtualUnlock</span> . Также стоит отметить, что попытка получить доступ к свободной или зарезервированной памяти приведет к исключению "нарушение доступа" (Access Violation Exception), поскольку эта память еще не может быть отображена в физическую память.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">Почему кто-то изобрел такой двусторонний процесс получения памяти? Как упоминалось ранее, шаблон последовательного доступа к памяти хорош по многим причинам. Пространство, состоящее из непрерывной последовательности страниц, предотвращает фрагментацию и, таким образом, оптимизирует использование TLB и позволяет избежать обхода каталогов страниц. Непрерывная память, конечно, также выгодна для использования кэша. Поэтому хорошо заранее зарезервировать немного большего пространства, даже если оно нам сейчас не нужно. Обычно именно так стеки потоков реализуются операционной системой: весь стек резервируется в адресном пространстве, и страницы фиксируются одна за другой при выполнении более глубоких вызовов методов, требуя больше места для параметров и локальных переменных.</p>
  </div>
</div>

<p class="justify-style">На <a href="#f-2-17">рисунке 2-17</a> графически изображены отношения между различными «наборами памяти» в виде перекрывающихся наборов:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Working set</span> - Рабочий набор: Это часть виртуального адресного пространства, которая в настоящее время находится в физической памяти. В свою очередь его можно разделить на</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Private working set</span> - Приватный рабочий набор: Состоит из зафиксированных (приватных) страниц в физической памяти</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Shared working set</span> - Общий рабочий набор: Состоит из страниц, которые являются общими для других процессов</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Private bytes</span> - Приватные байты: Все зафиксированные (приватные) страницы – как в физической, так и в выгружаемой памяти</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Virtual bytes</span> - Виртуальные байты: зарезервированная память в адресном пространстве (как зафиксированная, так и незафиксированная)</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Paged bytes</span> - Выгружаемые байты: Часть виртуальных байтов, хранящихся в файле подкачки</p>
  </li>
</ul>

<figure id="f-2-17" class="figure">
  <img src="content/img/2-17.png" class="img-fluid" alt="Figure 2-17" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-17. Связь между различными наборами памяти в рамках процесса в Windows</figcaption>
</figure>

<p class="justify-style">Довольно сложно, не правда ли? Возможно, теперь вы должны осознать, что ответ на вопрос «сколько памяти на самом деле занимает наш процесс .NET» не так уж и очевиден. На какой из этих показателей стоит обратить внимание? Ошибочно считается, что самым важным показателем является приватный рабочий набор, потому что он показывает, каково реальное влияние процесса на потребление важнейшей физической оперативной памяти. Однако, в случае утечки памяти, приватные байты будут расти, а может и не приватный рабочий набор. О том, как следить за этими показателями, вы узнаете в следующей главе. Вы также поймете, что де-факто отображается диспетчером задач в виде столбца памяти процесса.</p>

<p class="justify-style">Из-за своей внутренней структуры, когда Windows резервирует область памяти для адресного пространства процесса, она учитывает следующее ограничение: как начало области, так и ее размер должны быть кратны размеру системной страницы (обычно 4 КБ) и так называемой гранулярности выделения (обычно 64 КБ). На практике это означает, что начальный адрес и размер каждого зарезервированного региона кратны 64 КБ. Если вы хотите выделить меньше, остаток будет недоступен (непригоден для использования). Таким образом, правильное выравнивание и размер блоков имеют решающее значение для того, чтобы избежать пустой траты памяти.</p>

<p class="justify-style">Несмотря на то, что вы не управляете памятью на уровне виртуального API на ежедневной основе, эти знания могут помочь вам понять проблему выравнивания в коде CLR. Внимательный читатель может спросить, почему гранулярность выделения составляет 64 КБ, а размер страницы — 4 КБ. Рэймонд Чен (Raymond Chen), сотрудник Microsoft, ответил на этот вопрос в 2003 году [Почему гранулярность распределения адресного пространства составляет 64 КБ? – <a href="https://devblogs.microsoft.com/oldnewthing/20031008-00/?p=42223]" target="_blank">https://devblogs.microsoft.com/oldnewthing/20031008-00/?p=42223]</a>. И как обычно в таких случаях, ответ очень интересный. Гранулярность распределения в основном обусловлена историческими причинами. Ядро всего семейства современных операционных систем восходит к корням раннего ядра Windows NT. Она поддерживала ряд платформ, включая архитектуру DEC Alpha. И именно из-за разнообразия поддерживаемых платформ и было введено такое ограничение. И поскольку было обнаружено, что он не представляет неудобств для других платформ, предпочтительнее было иметь общий базовый код ядра. Более подробные объяснения вы найдете в упомянутой статье.</p>

<hr class="border-2">

<p id="chapter2-2-6" class="h4 p-2">Компоновка памяти в Windows</p>

<p class="justify-style">Теперь давайте углубимся в процесс .NET, работающий в Windows. Процесс содержит одну кучу процесса по умолчанию (в основном используемую внутренними функциями Windows) и любое количество необязательных куч (созданных с помощью API кучи). Одним из примеров такой ситуации является куча, созданная средой выполнения Microsoft C и используемая операторами C/C++, как упоминалось ранее. Существует три основных типа кучи:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Normal (NT) heap</span> - Нормальная куча (NT): используется обычными приложениями (не универсальной платформой Windows — UWP). Это провайдер предоставляющий базовый функционал управления блоками памяти.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Low-fragmentation heap</span> - Куча с низкой фрагментацией: дополнительный уровень над обычной функциональностью кучи, который управляет выделениями в предопределенных блоках различных размеров. Это предотвращает фрагментацию небольших данных и дополнительно делает доступ к ним немного быстрее благодаря внутренней оптимизации ОС.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Segment heap</span> - Сегментная куча: используется приложениями универсальной платформы Windows, которые предоставляют более сложные распределители (включая распределитель с низким уровнем фрагментации, аналогичный упомянутому ранее).</p>
  </li>
</ul>

<p class="justify-style">Как упоминалось ранее, структура адресного пространства процесса разделена на две части, где верхние адреса заняты ядром, а нижние адреса — пользователем (программой). Это показано на <a href="#f-2-18">рисунке 2-18</a> (32-битный слева, 64-битный справа). На 32-битных машинах, в зависимости от значения флага большого адреса, пользовательское пространство находится в нижних 2 или 3 ГБ. На современных 64-разрядных процессорах, поддерживающих 48-разрядную адресацию, как пользовательскому, так и ядерному пространству доступно 128 ТБ виртуальной памяти (8 ТБ в предыдущих версиях — Windows 8 и Server 2012).</p>

<p class="justify-style">В некотором приближении типичная структура пользовательского пространства программы .NET в Windows выглядит следующим образом:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style">Куча по умолчанию, упомянутая ранее.</p>
  </li>
  <li>
    <p class="justify-style">Большинство образов (exe, dlls) находятся по высоким адресам.</p>
  </li>
  <li>
    <p class="justify-style">Стеки потоков (о которых говорилось в предыдущей главе) располагаются где угодно. Каждый поток в процессе имеет собственную область стека потоков. Это относится к потокам CLR, которые просто оборачивают собственные системные потоки.</p>
  </li>
  <li>
    <p class="justify-style">Кучи GC, управляемые средой CLR для хранения создаваемых вами объектов .NET (это обычные страницы в номенклатуре Windows, зарезервированные и зафиксированные Virtual API).</p>
  </li>
  <li>
    <p class="justify-style">Различные частные кучи CLR для внутренних целей. Более подробно мы рассмотрим их в следующих главах.</p>
  </li>
  <li>
    <p class="justify-style">Также, конечно, довольно много свободного виртуального адресного пространства, включая огромные блоки порядка гигабайт и терабайт (в зависимости от архитектуры) где-то посередине виртуального адресного пространства.</p>
  </li>
</ul>

<figure id="f-2-18" class="figure">
  <img src="content/img/2-18.png" class="img-fluid" alt="Figure 2-18" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-18. Макет виртуального адресного пространства x86/ARM (32-разрядная версия) и x64 (64-разрядная версия) процесса в Windows, выполняющего управляемый код .NET</figcaption>
</figure>

<p class="justify-style">Начальный размер стека потока в Windows (как зарезервированный, так и изначально зафиксированный) берется из заголовка исполняемого файла (обычно называемого EXE-файлом). Для потоков, созданных вручную, размер стека также можно указать, вручную вызвав Windows CreateThread API.</p>

<p class="justify-style">То, как среда выполнения .NET вычисляет размер стека по умолчанию, довольно сложно. Значение по умолчанию составляет 1 МБ для типичной 32-битной компиляции и 4 МБ для типичной 64-битной компиляции. Данные стека довольно малы, а стек вызовов обычно довольно поверхностен (сотни вложенных вызовов встречаются довольно редко). Это делает 1 или 4 МБ хорошим значением по умолчанию.</p>

<p class="justify-style">Однако, если вы когда-либо сталкивались с StackOverflowException, вы просто столкнулись с этим барьером. Даже в этом случае это, скорее всего, связано с ошибкой программирования, например, бесконечной рекурсией. Если по какой-то причине вы пишете программу, которой необходимо хранить много данных в стеке, вы можете изменить заголовок двоичного файла. Исполняемые файлы .NET интерпретируются как обычные исполняемые файлы, поэтому это изменение будет отражено операционной системой. Мы увеличим этот предельный размер стека для этой цели в Главе 4.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">По соображениям безопасности был введен механизм рандомизации адресного пространства (ASLR), который делает все макеты, показанные на <a href="#f-2-18">рисунке 2-18</a>, только иллюстративными. Это приводит к тому, что все компоненты (двоичные изображения) размещаются случайным образом по всему адресному пространству, чтобы не повторять какой-либо общий шаблон, который может быть использован злоумышленником.</p>
  </div>
</div>

<p class="justify-style">Мы надеемся, что такой взгляд с высоты птичьего полета позволит вам лучше понять управление памятью CLR в контексте всей экосистемы Windows. Мы еще раз обратимся к этим знаниям при подробном описании структуры памяти CLR.</p>

<hr class="border-2">
<p id="chapter2-2-7" class="h4 p-2">Управление памятью в Linux</p>

<p class="justify-style">До недавнего времени любое упоминание Linux в книге о .NET ограничивалось проектом Mono. Но времена изменились. С появлением среды .NET Core (включая .NET 5+) больше невозможно пропустить системы, отличные от Windows. Более того, популярность запуска .NET на компьютерах, отличных от Windows, продолжает расти вместе с распространением Linux в мире контейнеров. Мы уделим много внимания реализации .NET Core и .NET 5+ во время выполнения. Поскольку Linux использует одну и ту же аппаратную технологию, включая pages, MMU и TLB, большая часть знаний уже рассмотрена в описаниях в предыдущих подразделах. Здесь мы остановимся только на интересных отличиях. Поскольку все больше и больше людей будут вынуждены развертывать приложения в этой среде, отличной от Windows .NET, очень полезно также понимать некоторые основы Linux.</p>

<p class="justify-style">Популярные и наиболее часто используемые дистрибутивы операционных систем Linux также используют концепцию виртуальной памяти. Их лимиты на процесс также очень похожи и обобщены в <a href="#t-2-5">таблице 2-5</a>.</p>

<br>
<figure id="t-2-5" class="figure">
  <table class="table table-striped table-hover border-1 ">
    <thead>
      <tr>
        <th scope="col">#</th>
        <th scope="col">Process Type</th>
        <th Scope="col">Linux 32-Bit</th>
        <th Scope="col">Linux 64-Bit</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th scope="row">1</th>
        <td>32-bit process</td>
        <td>3/1, 2/2, 1/3 GB</td>
        <td>-</td>
      </tr>
      <tr>
        <th scope="row">2</th>
        <td>64-bit process</td>
        <td>-</td>
        <td>128/128 TB*</td>
      </tr>
    </tbody>
  </table>
  <figcaption class="figure-caption">
    <p>Таблица 2-5. Ограничения на размер виртуального адресного пространства в Linux (пользователь/ядро)</p>
    <p>*Каноническая 48-битная адресация</p>
  </figcaption>
</figure>

<p class="justify-style">Как и в Windows, основным строительным блоком в Linux является страница, размер которой также обычно составляет 4 КБ. Страница может находиться в любом из трех различных состояний, перечисленных ниже:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Free</span> - Свободно: не назначено ни одному процессу или системе.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Allocated</span> - Выделено: Назначено процессу.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Shared</span> - Общий: зарезервирован для процесса, но может быть использован совместно с другими процессами. Обычно это относится к двоичным образам и сопоставленным в память файлам общесистемных библиотек и ресурсов.</p>
  </li>
</ul>

<p class="justify-style">Это делает более простым и понятным представление о потреблении памяти процессом, чем в случае операционной системы Windows. Как вы можете видеть, по сравнению с Windows, неявный этап резервирования страниц отсутствует, хотя он все еще существует явно. Linux имеет встроенный механизм ленивого выделения, который заботится об этом. Когда кто-то выделяет память в Linux, она рассматривается как выделенная, но никакие физические ресурсы не назначаются (следовательно, это похоже на резервирование в Windows). Фактическое назначение ресурсов (потребление физической памяти) не произойдет, пока оно фактически не понадобится путем доступа к этой конкретной области памяти. Если вы хотите заранее подготовить такие страницы в критических для производительности сценариях, вы можете «трогать» их, обращаясь к их памяти, например, считывая по крайней мере один байт.</p>

<p class="justify-style">Зная возможные статусы страниц, вы можете посмотреть, на какие категории делится память процесса в Linux. Вокруг этого довольно много путаницы. Многие инструменты на базе Linux говорят немного разные вещи по этому поводу. Использование памяти процесса можно измерить относительно следующих терминов:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Virtual</span> (отмечен некоторыми инструментами как vsz): Общий размер виртуального адресного пространства, зарезервированного на данный момент процессом. В популярном инструменте «top» он отображается в столбце VIRT.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Resident</span> (Resident Set Size, RSS): Пространство страниц, которые в данный момент находятся в физической памяти. Некоторые резидентные страницы могут совместно использоваться процессами (например, страницы с файловой поддержкой). Таким образом, это эквивалент «рабочего набора» на платформе Windows. В «top» это называется столбцом RES. Его можно дополнительно разделить на</p>
  </li>
  <ul class="bullet-list ms-1">
    <li>
      <p class="justify-style"><span class="fw-bold fst-italic">Private resident pages</span>: Это все анонимные резидентные страницы, зарезервированные для этого процесса (указанные счетчиком ядра MM_ANONPAGES). Это в некоторой степени соответствует «частному рабочему набору» в Windows.</p>
    </li>
    <li>
      <p class="justify-style"><span class="fw-bold fst-italic">Shared resident pages</span>: Это как поддерживаемые файлами (указанные счетчиком ядра MM_FILEPAGES), так и анонимные резидентные страницы процесса, соответствующие «общему рабочему набору». В «top» это называется памятью SHR.</p>
    </li>
  </ul>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Private</span>: Все частные страницы процесса. В инструменте «top» это столбец DATA. Обратите внимание, что это индикатор зарезервированной памяти и не говорит, какая ее часть уже была использована («тронута») и, таким образом, стала резидентной. Это соответствует «частным байтам» в Windows.</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">Swapped</span>: Часть виртуальной памяти, которая была сохранена в файле подкачки.</p>
  </li>
</ul>

<p class="justify-style">На <a href="#f-2-19">рисунке 2-19</a> графически отображена взаимосвязь между этими показателями в виде перекрывающихся наборов.</p>

<figure id="f-2-19" class="figure">
  <img src="content/img/2-19.png" class="img-fluid" alt="Figure 2-19" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-19. Взаимосвязь между различными наборами памяти в процессе на Linux</figcaption>
</figure>

<p class="justify-style">Довольно сложно. Как и в случае с Windows, ответ на вопрос, что потребляет память нашего процесса .NET, нетривиален. Самое разумное — посмотреть на измерение «частных резидентных страниц», поскольку оно показывает фактическое использование ценного ресурса ОЗУ процессом.</p>

<br>
<div class="card text-bg-info mb-3 bg-opacity-10">
  <div class="card-header"><i class="bi bi-info-square"></i> Примечание</div>
  <div class="card-body">
    <p class="card-text justify-style">В то время как в Windows гранулярность выделения составляет 64 КБ, в Linux она ограничивается только размером страницы, который в большинстве случаев составляет 4 КБ.</p>
  </div>
</div>

<hr class="border-2">

<p id="chapter2-2-8" class="h4 p-2">Компоновка памяти в Linux</p>

<p class="justify-style">Структура памяти процесса Linux очень похожа на ту, что была представлена ​​для Windows. Для 32-разрядной версии пространство пользователя составляет 3 ГБ, а пространство ядра — 1 ГБ. Эту точку разделения можно изменить с помощью параметра CONFIG_PAGE_OFFSET, настраиваемого во время сборки ядра. Для 64-разрядной версии разделение выполняется по аналогичному адресу, как в Windows (см. <a href="#f-2-20">рисунок 2-20</a>).</p>

<figure id="f-2-20" class="figure">
  <img src="content/img/2-20.png" class="img-fluid" alt="Figure 2-20" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-20. Структура виртуальной памяти x86/ARM (32-бит) и x64 (64-бит) процесса в Linux</figcaption>
</figure>

<p class="justify-style">Подобно Windows, система предоставляет API для работы со страницами памяти. Она содержит:</p>

<ul class="bullet-list ms-1">
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">mmap</span>: Для непосредственного управления страницами (включая файловые карты, общие и обычные, а также анонимные отображения, которые не связаны ни с одним файлом, но используются для хранения данных программы).</p>
  </li>
  <li>
    <p class="justify-style"><span class="fw-bold fst-italic">brk/sbrk</span>: Это ближайший эквивалент метода VirtualAlloc. Он позволяет вам устанавливать/увеличивать так называемый «программный перерыв», что на самом деле означает увеличение размера кучи.</p>
  </li>
</ul>

<p class="justify-style">Известные распределители C/C++ используют mmap или brk в зависимости от размера выделения. Этот порог можно настроить с помощью mallopt и параметра M_MMAP_THRESHOLD. Как вы увидите позже, CLR использует mmap с анонимными приватными страницами.</p>

<p class="justify-style">Есть одно существенное различие в обработке стека потоков между Linux и Windows. Поскольку нет двухэтапного резервирования памяти, стек просто расширяется по мере необходимости. Нет предварительного резервирования соответствующих страниц памяти. И поскольку следующие страницы создаются по мере необходимости, стек потоков не является непрерывной областью памяти.</p>

<hr class="border-2">

<p id="chapter2-2-9" class="h4 p-2">Влияние операционной системы</p>

<p class="justify-style">Есть ли какие-либо различия в управлении памятью, которые были учтены в кроссплатформенной версии Garbage Collector, включенной в CLR? В целом, код GC очень платформенно-независим, но по понятным причинам в какой-то момент должны быть сделаны системные вызовы. Менеджер памяти в обеих операционных системах работает схожим образом — он основан на виртуальной памяти, подкачке и схожем способе выделения памяти. Хотя, конечно, вызываемые системные API различны, концептуально никаких особых различий в коде нет, за исключением двух ситуаций, которые мы хотели бы сейчас описать.</p>

<p class="justify-style">Первое отличие уже упоминалось. В Linux нет двухэтапного способа резервирования и последующего выделения памяти. В Windows вы можете использовать системный вызов, чтобы сначала зарезервировать большой блок памяти. Это будет создание соответствующих системных структур без фактического использования физической памяти. Только при необходимости выполняется второй этап выделения диапазона памяти. Поскольку в Linux нет этого механизма, память может быть выделена только без «резервирования». Однако для имитации двухэтапного способа резервирования/выделения потребовался системный API. Для этой цели использовался популярный трюк. В Linux «резервирование» осуществляется путем выделения памяти с режимом доступа PROT_NONE, что фактически означает, что доступ к этой памяти запрещен. Однако в такой зарезервированной области вы можете затем снова выделить определенные под регионы с обычными правами, таким образом имитируя «выделение» памяти.</p>

<p class="justify-style">Второе отличие — так называемый механизм наблюдения за записью в память. Как вы увидите в последующих главах, сборщику мусора необходимо отслеживать, какие области памяти (страницы) были изменены. Для этой цели Windows предоставляет удобный API. При выделении страницы можно установить флаг MEM_WRITE_WATCH. Затем, используя API GetWriteWatch, можно получить список измененных страниц. При реализации .NET Core стало очевидно, что в Linux нет системного вызова для построения такого механизма наблюдения. По этой причине эту логику пришлось перенести на барьер записи (механизм, подробно описанный в главе 5), который поддерживается средой выполнения без какой-либо поддержки операционной системы.</p>

<hr class="border-2">

<p id="chapter2-3" class="h3 p-2">NUMA и группы ЦПУ</p>

<p class="justify-style">Есть еще один важный элемент головоломки, который нужно добавить к большой головоломке управления памятью. Симметричная многопроцессорность (SMP) означает, что компьютер имеет несколько идентичных ЦП, которые подключены к общей основной памяти. Они управляются одной операционной системой, которая может или не может обрабатывать все процессоры одинаково. Как вы знаете, у каждого ЦП есть свой собственный набор кэшей L1 и L2. Другими словами, у каждого ЦП есть некоторая выделенная локальная память, которая доступна намного быстрее, чем другие области памяти. Потоки и программы, работающие на разных ЦП, вероятно, будут совместно использовать некоторые данные, что не идеально, поскольку совместное использование данных через соединения ЦП вызывает значительные задержки. Вот где в игру вступает неоднородная архитектура памяти (NUMA). Это означает, что области памяти имеют разные характеристики производительности в зависимости от ЦП, который к ним обращается. И программное обеспечение (в основном операционная система, но опционально и сама программа) должно поддерживать NUMA, чтобы предпочесть использование этих локальных областей памяти более удаленным. Такая конфигурация проиллюстрирована на <a href="#f-2-21">рисунке 2-21</a>.</p>

<figure id="f-2-21" class="figure">
  <img src="content/img/2-21.png" class="img-fluid" alt="Figure 2-21" max-width="600">
  <figcaption class="figure-caption">Рисунок 2-21. Пример конфигурации NUMA с восемью процессорами, сгруппированными в два узла NUMA</figcaption>
</figure>

<p class="justify-style">Такие дополнительные накладные расходы на доступ к нелокальной памяти называются фактором NUMA. Поскольку прямое соединение всех ЦП было бы очень дорогим, каждый ЦП обычно имеет соединения только с двумя или тремя другими ЦП. Для доступа к удаленной памяти необходимо сделать несколько переходов между процессорами, что увеличивает задержку. Чем больше ЦП, тем более важен фактор NUMA, если используется нелокальная память. Существуют также системы со смешанным подходом, где группы процессоров имеют некоторую общую память, и память неравномерна между этими группами с большим фактором NUMA между ними. Это фактически наиболее распространенный подход в системе с поддержкой NUMA. ЦП группируются в более мелкие системы, называемые узлами NUMA. Каждый узел NUMA имеет свои собственные процессоры и память с небольшим фактором NUMA из-за аппаратной организации. Узлы NUMA, конечно, взаимосвязаны, но передача данных между ними подразумевает большие накладные расходы.</p>

<p class="justify-style">Основное требование для осведомленности системы и программного кода о NUMA — это использование локальной DRAM того узла NUMA, на котором выполняется процесс. Однако это может привести к несбалансированному состоянию, если некоторые процессы потребляют намного больше памяти, чем другие. Это заставляет задаться вопросом: осведомлена ли .NET о NUMA? Простой ответ: да, она осведомлена! Память выделяется диспетчером памяти .NET (GC) на соответствующем узле NUMA, поэтому память будет находиться "близко" к потокам, выполняющим управляемый код. При балансировке кучи GC приоритет отдается распределению памяти в кучах, расположенных на том же узле NUMA. Теоретически осведомленность о NUMA можно отключить с помощью параметра GCNumaAware в секции конфигурации среды выполнения, хотя трудно представить причину, по которой кто-то захотел бы это сделать.</p>

<p class="justify-style">Однако существуют два других важных параметра приложения, показанных в <a href="#l-2-7">Листинге 2-7</a>, которые связаны с так называемыми группами процессоров. На системах Windows с более чем 64 логическими процессорами они объединяются в упомянутые группы CPU. По умолчанию процессы ограничены одной группой CPU, что означает, что они не будут использовать все процессоры, доступные в системе.</p>

<p class="justify-style">Вы можете включить осведомленность о группах CPU в средах выполнения .NET на базе Windows (см. <a href="#l-2-7">Листинг 2-7</a>), что является важным в средах с более чем 64 логическими процессорами, если вы хотите, чтобы ваш процесс использовал все ресурсы машины.</p>

<br>
<figure id="l-2-7" class="figure">
  <pre class="code border border-secondary">
    <code class="language-csharp">
      &lt;configuration&gt;
        &lt;runtime&gt;
          &lt;GCCpuGroup enabled="true"/&gt;
          &lt;gcServer enabled="true"/&gt;
        &lt;/runtime&gt;
      &lt;/configuration&gt;
    </code>
  </pre>
  <figcaption class="figure-caption">Листинг 2-7. Настройка осведомленности о группе процессоров в среде выполнения .NET</figcaption>
</figure>

<p class="justify-style">Параметр <code class="fs-6 fst-italic">GCCpuGroup</code> определяет, должен ли сборщик мусора (Garbage Collector) поддерживать группы CPU путем создания внутренних потоков сборки мусора во всех доступных группах и учитывать все доступные ядра при создании и управлении кучами. Этот параметр следует включать одновременно с параметром <code class="fs-6 fst-italic">gcServer</code>.</p>

<hr class="border-2">

<p id="chapter2-4" class="h3 p-2">Резюме</p>

<p class="justify-style">Вы проделали долгий путь в этой главе. Мы кратко рассмотрели наиболее важные механизмы управления аппаратным обеспечением и системной памятью. Это знание, вместе с теоретическим введением из предыдущей главы, должно дать вам гораздо более широкий контекст для лучшего понимания управления памятью в .NET. В каждой последующей главе мы будем всё дальше уходить от общих аспектов аппаратного обеспечения и теоретических положений, углубляясь в особенности среды выполнения .NET.</p>

<hr class="border-2">

<p id="chapter2-4-1" class="h4 p-2">Правило 2 - Следует избегать случайного доступа, поощрять последовательный доступ</p>

<p class="justify-style"><span class="fw-bold fst-italic">Применимость</span>: В основном низкоуровневый, ориентированный на производительность код.</p>

<p class="justify-style"><span class="fw-bold fst-italic">Обоснование</span>: Из-за внутренних механизмов на многих уровнях, включая конструкцию оперативной памяти и кэша процессора, последовательный доступ определенно более эффективен. Доступ к удаленной памяти в DRAM требует значительно больше тактов процессора, чем доступ к кэшу. Процессор загружает данные блоками по 64 байта, называемыми строками кэша. Каждый доступ к памяти объемом меньше 64 байт представляет собой расточительное использование дорогих ресурсов. Кроме того, случайные паттерны доступа снижают вероятность работы механизма предварительной загрузки кэша. Процессору нет шансов обнаружить прогнозируемый паттерн при случайном доступе к памяти. Под случайностью мы не подразумеваем полную хаотичность, а скорее любой доступ, который не соответствует обнаруживаемому паттерну.</p>

<p class="justify-style"><span class="fw-bold fst-italic">Как применять</span>: Очевидно, противоположностью случайного доступа является последовательный доступ, поэтому старайтесь всегда использовать его. Если вы работаете с большим объемом данных, имеет смысл рассмотреть возможность их упаковки в массивы для обеспечения непрерывности памяти. Итерация по двусвязным спискам может быть примером типичного неструктурированного доступа. Мы более подробно рассмотрим этот аспект доступа к памяти в Главе 13 при описании так называемого данных-ориентированного дизайна.</p>

<hr class="border-2">

<p id="chapter2-4-2" class="h4 p-2">Правило 3 - Улучшайте пространственную и временную локальность данных</p>

<p class="justify-style"><span class="fw-bold fst-italic">Применимость</span>: В основном низкоуровневый, ориентированный на производительность код.</p>

<p class="justify-style"><span class="fw-bold fst-italic">Обоснование</span>: Пространственная и временная локальности являются основными принципами эффективного кэша. Если эти принципы соблюдены, кэш используется правильно и помогает достичь лучшей производительности. Наоборот, если вы нарушаете временную или пространственную локальность, это приведет к значительному падению производительности.</p>

<p class="justify-style"><span class="fw-bold fst-italic">Как применять</span>: Проектируйте свои структуры данных так, чтобы учитывать локальность ваших данных и максимизировать их повторное использование во времени. Как вы могли видеть из приведенных примеров, распределенный и случайный доступ к данным может быть в несколько раз медленнее. Иногда, в самых продвинутых и высокопроизводительных частях программы, это означает внесение не интуитивных изменений, которые будут рассмотрены в подходе данных-ориентированного дизайна в Главе 13. Более широко говоря, это часто сводится к тому, чтобы ваши структуры данных были достаточно маленькими, предварительно выделенными и многократно используемыми.</p>

<hr class="border-2">

<p id="chapter2-4-3" class="h4 p-2">Правило 4 - Рассмотрите более продвинутые возможности</p>

<p class="justify-style"><span class="fw-bold fst-italic">Применимость</span>: Чрезвычайно низкоуровневый, ориентированный на производительность код.</p>

<p class="justify-style"><span class="fw-bold fst-italic">Обоснование</span>: Среда выполнения .NET реализована наиболее универсальным образом. Это гарантирует корректную работу в широком спектре возможных сценариев. Однако, при написании вашего приложения, вы должны знать свои "горячие точки", где следует сосредоточиться на производительности. Возможно, вам придется писать фрагменты кода, связанные с управлением памятью, которые работают чрезвычайно быстро. В таком случае вы можете рассмотреть использование некоторых более продвинутых механизмов операционной системы. Такие механизмы, вероятно, понадобятся лишь малой части разработчиков .NET по всему миру. Если вы пишете библиотеки, связанные с памятью, такие как сериализаторы, буферы сообщений или любой вид крайне быстрого обработчика событий, возможно, вы сможете получить выгоду от использования некоторых из упомянутых здесь низкоуровневых системных API (например, не временного доступа к памяти).</p>

<p class="justify-style"><span class="fw-bold fst-italic">Как применять</span>: Это потребует написания очень сложного кода. Написание такого кода будет болезненным процессом, и, вероятно, никто не захочет поддерживать его — кроме вас. Поскольку он будет использовать низкоуровневый API операционной системы, это может также вызвать проблемы после обновления или изменения версии операционной системы. Также весьма маловероятно, что вам вообще потребуется такое низкоуровневое управление памятью, поскольку оно потребует экстраординарной осторожности при программировании. И очень легко допустить ошибку, которая вместо увеличения производительности резко ее снизит.</p>

<p class="justify-style"><span class="fw-bold fst-italic">Рекомендации</span>: Тщательно прочтите эту книгу. Затем внимательно изучите книги о внутреннем устройстве конкретных операционных систем. И только после этого попробуйте использовать продвинутые механизмы, такие как большие страницы, не временные операции и другие, упомянутые в этой главе.</p>

    